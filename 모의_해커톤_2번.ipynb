{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVb1jb7N/+/b2yPu9RA3Lu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Donggeon2960/LGAIMER-PRACTICE/blob/main/%EB%AA%A8%EC%9D%98_%ED%95%B4%EC%BB%A4%ED%86%A4_2%EB%B2%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¸”ëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "InXWGGY9xYml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# ìž„ì‹  ì„±ê³µ ì—¬ë¶€ ì˜ˆì¸¡ ëª¨ë¸ - ì„±ëŠ¥ í–¥ìƒ ë²„ì „\n",
        "# 1ë‹¨ê³„: ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "# ====================================================================\n",
        "\n",
        "# ê¸°ë³¸ ë°ì´í„° ì²˜ë¦¬ ë° ìˆ˜ì¹˜ ì—°ì‚°\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ì‹œê°í™”\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler, RobustScaler, MinMaxScaler,\n",
        "    LabelEncoder, OrdinalEncoder, OneHotEncoder,\n",
        "    TargetEncoder, PowerTransformer, QuantileTransformer\n",
        ")\n",
        "from sklearn.feature_selection import (\n",
        "    SelectKBest, f_classif, chi2, mutual_info_classif,\n",
        "    RFE, RFECV, SelectFromModel\n",
        ")\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.manifold import TSNE\n",
        "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (IterativeImputerëŠ” experimentalì´ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬)\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "try:\n",
        "    from sklearn.experimental import enable_iterative_imputer\n",
        "    from sklearn.impute import IterativeImputer\n",
        "    print(\"âœ“ IterativeImputer ì‚¬ìš© ê°€ëŠ¥\")\n",
        "except ImportError:\n",
        "    print(\"âš  IterativeImputerëŠ” sklearn 0.21+ ë²„ì „ì—ì„œ ì‚¬ìš© ê°€ëŠ¥\")\n",
        "\n",
        "# ëª¨ë¸ë“¤ - íŠ¸ë¦¬ ê¸°ë°˜\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, ExtraTreesClassifier,\n",
        "    GradientBoostingClassifier, AdaBoostClassifier,\n",
        "    VotingClassifier, BaggingClassifier\n",
        ")\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# ëª¨ë¸ë“¤ - ê¸°íƒ€ ì•Œê³ ë¦¬ì¦˜\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# ê³ ì„±ëŠ¥ ë¶€ìŠ¤íŒ… ëª¨ë¸ë“¤\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    print(\"âœ“ LightGBM ì‚¬ìš© ê°€ëŠ¥\")\n",
        "except ImportError:\n",
        "    print(\"âš  LightGBM ì„¤ì¹˜ í•„ìš”: pip install lightgbm\")\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    print(\"âœ“ XGBoost ì‚¬ìš© ê°€ëŠ¥\")\n",
        "except ImportError:\n",
        "    print(\"âš  XGBoost ì„¤ì¹˜ í•„ìš”: pip install xgboost\")\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "    print(\"âœ“ CatBoost ì‚¬ìš© ê°€ëŠ¥\")\n",
        "except ImportError:\n",
        "    print(\"âš  CatBoost ì„¤ì¹˜ í•„ìš”: pip install catboost\")\n",
        "\n",
        "# ëª¨ë¸ í‰ê°€ ë° ê²€ì¦\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, StratifiedKFold,\n",
        "    GridSearchCV, RandomizedSearchCV, validation_curve,\n",
        "    learning_curve\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, precision_recall_curve,\n",
        "    confusion_matrix, classification_report,\n",
        "    log_loss, average_precision_score\n",
        ")\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
        "try:\n",
        "    import optuna\n",
        "    print(\"âœ“ Optuna ì‚¬ìš© ê°€ëŠ¥\")\n",
        "except ImportError:\n",
        "    print(\"âš  Optuna ì„¤ì¹˜ í•„ìš”: pip install optuna\")\n",
        "\n",
        "# ëª¨ë¸ í•´ì„\n",
        "try:\n",
        "    import shap\n",
        "    print(\"âœ“ SHAP ì‚¬ìš© ê°€ëŠ¥\")\n",
        "except ImportError:\n",
        "    print(\"âš  SHAP ì„¤ì¹˜ í•„ìš”: pip install shap\")\n",
        "\n",
        "# ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "    from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours\n",
        "    from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "    print(\"âœ“ imbalanced-learn ì‚¬ìš© ê°€ëŠ¥\")\n",
        "except ImportError:\n",
        "    print(\"âš  imbalanced-learn ì„¤ì¹˜ í•„ìš”: pip install imbalanced-learn\")\n",
        "\n",
        "# ìžë™í™”ëœ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\n",
        "try:\n",
        "    import featuretools as ft\n",
        "    print(\"âœ“ Featuretools ì‚¬ìš© ê°€ëŠ¥\")\n",
        "except ImportError:\n",
        "    print(\"âš  Featuretools ì„¤ì¹˜ í•„ìš”: pip install featuretools\")\n",
        "\n",
        "# ê¸°íƒ€ ìœ í‹¸ë¦¬í‹°\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "import os\n",
        "import gc\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# ì„¤ì •\n",
        "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# ìž¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸš€ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"ðŸ“Š Pandas: {pd.__version__}\")\n",
        "print(f\"ðŸ”¢ NumPy: {np.__version__}\")\n",
        "print(f\"ðŸ¤– Scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"ðŸ“ˆ Matplotlib: {matplotlib.__version__}\")\n",
        "print(f\"ðŸŽ¨ Seaborn: {sns.__version__}\")\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… 1ë‹¨ê³„ ì™„ë£Œ: ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\")\n",
        "print(\"ðŸ”„ ë‹¤ìŒ ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA)\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_K49FsdxYSc",
        "outputId": "df593815-b189-4ebd-dfcb-1d227543d918"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ IterativeImputer ì‚¬ìš© ê°€ëŠ¥\n",
            "âœ“ LightGBM ì‚¬ìš© ê°€ëŠ¥\n",
            "âœ“ XGBoost ì‚¬ìš© ê°€ëŠ¥\n",
            "âš  CatBoost ì„¤ì¹˜ í•„ìš”: pip install catboost\n",
            "âš  Optuna ì„¤ì¹˜ í•„ìš”: pip install optuna\n",
            "âœ“ SHAP ì‚¬ìš© ê°€ëŠ¥\n",
            "âœ“ imbalanced-learn ì‚¬ìš© ê°€ëŠ¥\n",
            "âš  Featuretools ì„¤ì¹˜ í•„ìš”: pip install featuretools\n",
            "======================================================================\n",
            "ðŸš€ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
            "======================================================================\n",
            "ðŸ“Š Pandas: 2.2.2\n",
            "ðŸ”¢ NumPy: 2.0.2\n",
            "ðŸ¤– Scikit-learn: 1.6.1\n",
            "ðŸ“ˆ Matplotlib: 3.10.0\n",
            "ðŸŽ¨ Seaborn: 0.13.2\n",
            "======================================================================\n",
            "âœ… 1ë‹¨ê³„ ì™„ë£Œ: ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
            "ðŸ”„ ë‹¤ìŒ ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA)\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA)"
      ],
      "metadata": {
        "id": "1HqjFIuOxZAN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0aqd3lHt7KL",
        "outputId": "452a6111-de1b-49d1-a601-90667fa7787c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ðŸ“‚ 2ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì‹œìž‘\n",
            "======================================================================\n",
            "ðŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...\n",
            "âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\n",
            "   ðŸ“ˆ Train ë°ì´í„°: (256351, 69)\n",
            "   ðŸ“‰ Test ë°ì´í„°: (90067, 68)\n",
            "\n",
            "==================================================\n",
            "ðŸ“‹ ê¸°ë³¸ ë°ì´í„° ì •ë³´\n",
            "==================================================\n",
            "\n",
            "ðŸ” Train ë°ì´í„° ê¸°ë³¸ ì •ë³´:\n",
            "   - í–‰ ìˆ˜: 256,351\n",
            "   - ì—´ ìˆ˜: 69\n",
            "   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 528.25 MB\n",
            "\n",
            "ðŸ” Test ë°ì´í„° ê¸°ë³¸ ì •ë³´:\n",
            "   - í–‰ ìˆ˜: 90,067\n",
            "   - ì—´ ìˆ˜: 68\n",
            "   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 184.74 MB\n",
            "\n",
            "ðŸ“ ì „ì²´ ì»¬ëŸ¼ ëª©ë¡ (69ê°œ):\n",
            "    1. ID\n",
            "    2. ì‹œìˆ  ì‹œê¸° ì½”ë“œ\n",
            "    3. ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´\n",
            "    4. ìž„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ìž„ì‹  ê²½ê³¼ ì—°ìˆ˜\n",
            "    5. ì‹œìˆ  ìœ í˜•\n",
            "    6. íŠ¹ì • ì‹œìˆ  ìœ í˜•\n",
            "    7. ë°°ëž€ ìžê·¹ ì—¬ë¶€\n",
            "    8. ë°°ëž€ ìœ ë„ ìœ í˜•\n",
            "    9. ë‹¨ì¼ ë°°ì•„ ì´ì‹ ì—¬ë¶€\n",
            "   10. ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€\n",
            "   11. ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€\n",
            "   12. ë‚¨ì„± ì£¼ ë¶ˆìž„ ì›ì¸\n",
            "   13. ë‚¨ì„± ë¶€ ë¶ˆìž„ ì›ì¸\n",
            "   14. ì—¬ì„± ì£¼ ë¶ˆìž„ ì›ì¸\n",
            "   15. ì—¬ì„± ë¶€ ë¶ˆìž„ ì›ì¸\n",
            "   16. ë¶€ë¶€ ì£¼ ë¶ˆìž„ ì›ì¸\n",
            "   17. ë¶€ë¶€ ë¶€ ë¶ˆìž„ ì›ì¸\n",
            "   18. ë¶ˆëª…í™• ë¶ˆìž„ ì›ì¸\n",
            "   19. ë¶ˆìž„ ì›ì¸ - ë‚œê´€ ì§ˆí™˜\n",
            "   20. ë¶ˆìž„ ì›ì¸ - ë‚¨ì„± ìš”ì¸\n",
            "   21. ë¶ˆìž„ ì›ì¸ - ë°°ëž€ ìž¥ì• \n",
            "   22. ë¶ˆìž„ ì›ì¸ - ì—¬ì„± ìš”ì¸\n",
            "   23. ë¶ˆìž„ ì›ì¸ - ìžê¶ê²½ë¶€ ë¬¸ì œ\n",
            "   24. ë¶ˆìž„ ì›ì¸ - ìžê¶ë‚´ë§‰ì¦\n",
            "   25. ë¶ˆìž„ ì›ì¸ - ì •ìž ë†ë„\n",
            "   26. ë¶ˆìž„ ì›ì¸ - ì •ìž ë©´ì—­í•™ì  ìš”ì¸\n",
            "   27. ë¶ˆìž„ ì›ì¸ - ì •ìž ìš´ë™ì„±\n",
            "   28. ë¶ˆìž„ ì›ì¸ - ì •ìž í˜•íƒœ\n",
            "   29. ë°°ì•„ ìƒì„± ì£¼ìš” ì´ìœ \n",
            "   30. ì´ ì‹œìˆ  íšŸìˆ˜\n",
            "   31. í´ë¦¬ë‹‰ ë‚´ ì´ ì‹œìˆ  íšŸìˆ˜\n",
            "   32. IVF ì‹œìˆ  íšŸìˆ˜\n",
            "   33. DI ì‹œìˆ  íšŸìˆ˜\n",
            "   34. ì´ ìž„ì‹  íšŸìˆ˜\n",
            "   35. IVF ìž„ì‹  íšŸìˆ˜\n",
            "   36. DI ìž„ì‹  íšŸìˆ˜\n",
            "   37. ì´ ì¶œì‚° íšŸìˆ˜\n",
            "   38. IVF ì¶œì‚° íšŸìˆ˜\n",
            "   39. DI ì¶œì‚° íšŸìˆ˜\n",
            "   40. ì´ ìƒì„± ë°°ì•„ ìˆ˜\n",
            "   41. ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜\n",
            "   42. ë¯¸ì„¸ì£¼ìž…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜\n",
            "   43. ì´ì‹ëœ ë°°ì•„ ìˆ˜\n",
            "   44. ë¯¸ì„¸ì£¼ìž… ë°°ì•„ ì´ì‹ ìˆ˜\n",
            "   45. ì €ìž¥ëœ ë°°ì•„ ìˆ˜\n",
            "   46. ë¯¸ì„¸ì£¼ìž… í›„ ì €ìž¥ëœ ë°°ì•„ ìˆ˜\n",
            "   47. í•´ë™ëœ ë°°ì•„ ìˆ˜\n",
            "   48. í•´ë™ ë‚œìž ìˆ˜\n",
            "   49. ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜\n",
            "   50. ì €ìž¥ëœ ì‹ ì„  ë‚œìž ìˆ˜\n",
            "   51. í˜¼í•©ëœ ë‚œìž ìˆ˜\n",
            "   52. íŒŒíŠ¸ë„ˆ ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜\n",
            "   53. ê¸°ì¦ìž ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜\n",
            "   54. ë‚œìž ì¶œì²˜\n",
            "   55. ì •ìž ì¶œì²˜\n",
            "   56. ë‚œìž ê¸°ì¦ìž ë‚˜ì´\n",
            "   57. ì •ìž ê¸°ì¦ìž ë‚˜ì´\n",
            "   58. ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\n",
            "   59. ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\n",
            "   60. ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\n",
            "   61. ëŒ€ë¦¬ëª¨ ì—¬ë¶€\n",
            "   62. PGD ì‹œìˆ  ì—¬ë¶€\n",
            "   63. PGS ì‹œìˆ  ì—¬ë¶€\n",
            "   64. ë‚œìž ì±„ì·¨ ê²½ê³¼ì¼\n",
            "   65. ë‚œìž í•´ë™ ê²½ê³¼ì¼\n",
            "   66. ë‚œìž í˜¼í•© ê²½ê³¼ì¼\n",
            "   67. ë°°ì•„ ì´ì‹ ê²½ê³¼ì¼\n",
            "   68. ë°°ì•„ í•´ë™ ê²½ê³¼ì¼\n",
            "   69. ìž„ì‹  ì„±ê³µ ì—¬ë¶€\n",
            "\n",
            "ðŸ·ï¸  ë°ì´í„° íƒ€ìž… ë¶„í¬:\n",
            "   float64: 29ê°œ\n",
            "   object: 21ê°œ\n",
            "   int64: 19ê°œ\n",
            "\n",
            "ðŸ”‘ ID ì»¬ëŸ¼ ë°œê²¬ - ì œê±° ì§„í–‰\n",
            "   âœ… ID ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n",
            "   ðŸ“ˆ Train: (256351, 68), Test: (90067, 67)\n",
            "\n",
            "ðŸŽ¯ íƒ€ê²Ÿ ë³€ìˆ˜ 'ìž„ì‹  ì„±ê³µ ì—¬ë¶€' ë¶„ë¦¬ ì™„ë£Œ\n",
            "   ðŸ“Š íŠ¹ì„± ë°ì´í„°: (256351, 67)\n",
            "   ðŸŽ¯ íƒ€ê²Ÿ ë°ì´í„°: (256351,)\n",
            "\n",
            "==================================================\n",
            "ðŸŽ¯ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ ë¶„ì„\n",
            "==================================================\n",
            "ðŸ“Š í´ëž˜ìŠ¤ ë¶„í¬:\n",
            "   í´ëž˜ìŠ¤ 0: 190,123ê°œ (74.17%)\n",
            "   í´ëž˜ìŠ¤ 1: 66,228ê°œ (25.83%)\n",
            "\n",
            "âš–ï¸  í´ëž˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨: 2.87:1\n",
            "   âš ï¸  ë¶ˆê· í˜• ë°ì´í„° - SMOTE ë“± ìƒ˜í”Œë§ ê¸°ë²• ê³ ë ¤ í•„ìš”\n",
            "\n",
            "==================================================\n",
            "ðŸ” ê²°ì¸¡ì¹˜ ë¶„ì„\n",
            "==================================================\n",
            "ðŸ“ˆ Train ë°ì´í„° ê²°ì¸¡ì¹˜:\n",
            "   ê²°ì¸¡ì¹˜ê°€ ìžˆëŠ” ì»¬ëŸ¼: 31ê°œ\n",
            "                       ê²°ì¸¡ì¹˜_ê°œìˆ˜  ê²°ì¸¡ì¹˜_ë¹„ìœ¨(%)\n",
            "ë‚œìž í•´ë™ ê²½ê³¼ì¼              254915  99.439831\n",
            "PGS ì‹œìˆ  ì—¬ë¶€              254422  99.247516\n",
            "PGD ì‹œìˆ  ì—¬ë¶€              254172  99.149994\n",
            "ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€       253633  98.939735\n",
            "ìž„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ìž„ì‹  ê²½ê³¼ ì—°ìˆ˜  246981  96.344855\n",
            "ë°°ì•„ í•´ë™ ê²½ê³¼ì¼              215982  84.252451\n",
            "ë‚œìž ì±„ì·¨ ê²½ê³¼ì¼               57488  22.425503\n",
            "ë‚œìž í˜¼í•© ê²½ê³¼ì¼               53735  20.961494\n",
            "ë°°ì•„ ì´ì‹ ê²½ê³¼ì¼               43566  16.994667\n",
            "ì €ìž¥ëœ ë°°ì•„ ìˆ˜                 6291   2.454057\n",
            "\n",
            "ðŸ“‰ Test ë°ì´í„° ê²°ì¸¡ì¹˜:\n",
            "   ê²°ì¸¡ì¹˜ê°€ ìžˆëŠ” ì»¬ëŸ¼: 30ê°œ\n",
            "                       ê²°ì¸¡ì¹˜_ê°œìˆ˜  ê²°ì¸¡ì¹˜_ë¹„ìœ¨(%)\n",
            "ë‚œìž í•´ë™ ê²½ê³¼ì¼               89575  99.453740\n",
            "PGS ì‹œìˆ  ì—¬ë¶€               89396  99.254999\n",
            "PGD ì‹œìˆ  ì—¬ë¶€               89286  99.132868\n",
            "ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€        89134  98.964104\n",
            "ìž„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ìž„ì‹  ê²½ê³¼ ì—°ìˆ˜   86770  96.339392\n",
            "ë°°ì•„ í•´ë™ ê²½ê³¼ì¼               76117  84.511530\n",
            "ë‚œìž ì±„ì·¨ ê²½ê³¼ì¼               19949  22.149067\n",
            "ë‚œìž í˜¼í•© ê²½ê³¼ì¼               18579  20.627977\n",
            "ë°°ì•„ ì´ì‹ ê²½ê³¼ì¼               15246  16.927398\n",
            "ì €ìž¥ëœ ë°°ì•„ ìˆ˜                 2176   2.415979\n",
            "\n",
            "==================================================\n",
            "ðŸ‘€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
            "==================================================\n",
            "ðŸ“Š Train ë°ì´í„° ìƒìœ„ 5í–‰:\n",
            "  ì‹œìˆ  ì‹œê¸° ì½”ë“œ ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´  ìž„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ìž„ì‹  ê²½ê³¼ ì—°ìˆ˜ ì‹œìˆ  ìœ í˜• íŠ¹ì • ì‹œìˆ  ìœ í˜•  ë°°ëž€ ìžê·¹ ì—¬ë¶€  \\\n",
            "0   TRZKPL  ë§Œ18-34ì„¸                    NaN   IVF     ICSI         1   \n",
            "1   TRYBLT  ë§Œ45-50ì„¸                    NaN   IVF     ICSI         0   \n",
            "2   TRVNRY  ë§Œ18-34ì„¸                    NaN   IVF      IVF         1   \n",
            "3   TRJXFG  ë§Œ35-37ì„¸                    NaN   IVF     ICSI         1   \n",
            "4   TRVNRY  ë§Œ18-34ì„¸                    NaN   IVF     ICSI         1   \n",
            "\n",
            "     ë°°ëž€ ìœ ë„ ìœ í˜•  ë‹¨ì¼ ë°°ì•„ ì´ì‹ ì—¬ë¶€  ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€  ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€  ë‚¨ì„± ì£¼ ë¶ˆìž„ ì›ì¸  \\\n",
            "0  ê¸°ë¡ë˜ì§€ ì•Šì€ ì‹œí–‰          0.0               NaN               0.0           0   \n",
            "1      ì•Œ ìˆ˜ ì—†ìŒ          0.0               NaN               0.0           0   \n",
            "2  ê¸°ë¡ë˜ì§€ ì•Šì€ ì‹œí–‰          0.0               NaN               0.0           0   \n",
            "3  ê¸°ë¡ë˜ì§€ ì•Šì€ ì‹œí–‰          0.0               NaN               0.0           0   \n",
            "4  ê¸°ë¡ë˜ì§€ ì•Šì€ ì‹œí–‰          0.0               NaN               0.0           0   \n",
            "\n",
            "   ë‚¨ì„± ë¶€ ë¶ˆìž„ ì›ì¸  ì—¬ì„± ì£¼ ë¶ˆìž„ ì›ì¸  ì—¬ì„± ë¶€ ë¶ˆìž„ ì›ì¸  ë¶€ë¶€ ì£¼ ë¶ˆìž„ ì›ì¸  ë¶€ë¶€ ë¶€ ë¶ˆìž„ ì›ì¸  ë¶ˆëª…í™• ë¶ˆìž„ ì›ì¸  \\\n",
            "0           0           0           0           0           0          0   \n",
            "1           0           0           0           0           0          0   \n",
            "2           0           0           0           0           0          0   \n",
            "3           0           0           0           0           0          0   \n",
            "4           0           0           0           0           0          0   \n",
            "\n",
            "   ë¶ˆìž„ ì›ì¸ - ë‚œê´€ ì§ˆí™˜  ë¶ˆìž„ ì›ì¸ - ë‚¨ì„± ìš”ì¸  ë¶ˆìž„ ì›ì¸ - ë°°ëž€ ìž¥ì•   ë¶ˆìž„ ì›ì¸ - ì—¬ì„± ìš”ì¸  \\\n",
            "0              0              1              1              0   \n",
            "1              0              0              0              0   \n",
            "2              0              1              0              0   \n",
            "3              0              1              0              0   \n",
            "4              1              0              0              0   \n",
            "\n",
            "   ë¶ˆìž„ ì›ì¸ - ìžê¶ê²½ë¶€ ë¬¸ì œ  ë¶ˆìž„ ì›ì¸ - ìžê¶ë‚´ë§‰ì¦  ë¶ˆìž„ ì›ì¸ - ì •ìž ë†ë„  ë¶ˆìž„ ì›ì¸ - ì •ìž ë©´ì—­í•™ì  ìš”ì¸  \\\n",
            "0                0              0              0                   0   \n",
            "1                0              0              0                   0   \n",
            "2                0              0              0                   0   \n",
            "3                0              0              0                   0   \n",
            "4                0              1              0                   0   \n",
            "\n",
            "   ë¶ˆìž„ ì›ì¸ - ì •ìž ìš´ë™ì„±  ë¶ˆìž„ ì›ì¸ - ì •ìž í˜•íƒœ ë°°ì•„ ìƒì„± ì£¼ìš” ì´ìœ  ì´ ì‹œìˆ  íšŸìˆ˜ í´ë¦¬ë‹‰ ë‚´ ì´ ì‹œìˆ  íšŸìˆ˜ IVF ì‹œìˆ  íšŸìˆ˜  \\\n",
            "0               0              0      í˜„ìž¬ ì‹œìˆ ìš©      0íšŒ            0íšŒ        0íšŒ   \n",
            "1               0              0      í˜„ìž¬ ì‹œìˆ ìš©      0íšŒ            0íšŒ        0íšŒ   \n",
            "2               0              0      í˜„ìž¬ ì‹œìˆ ìš©      1íšŒ            0íšŒ        1íšŒ   \n",
            "3               0              0      í˜„ìž¬ ì‹œìˆ ìš©      1íšŒ            1íšŒ        1íšŒ   \n",
            "4               0              0      í˜„ìž¬ ì‹œìˆ ìš©      0íšŒ            0íšŒ        0íšŒ   \n",
            "\n",
            "  DI ì‹œìˆ  íšŸìˆ˜ ì´ ìž„ì‹  íšŸìˆ˜ IVF ìž„ì‹  íšŸìˆ˜ DI ìž„ì‹  íšŸìˆ˜ ì´ ì¶œì‚° íšŸìˆ˜ IVF ì¶œì‚° íšŸìˆ˜ DI ì¶œì‚° íšŸìˆ˜  ì´ ìƒì„± ë°°ì•„ ìˆ˜  \\\n",
            "0       0íšŒ      0íšŒ        0íšŒ       0íšŒ      0íšŒ        0íšŒ       0íšŒ        4.0   \n",
            "1       0íšŒ      0íšŒ        0íšŒ       0íšŒ      0íšŒ        0íšŒ       0íšŒ        0.0   \n",
            "2       0íšŒ      0íšŒ        0íšŒ       0íšŒ      0íšŒ        0íšŒ       0íšŒ        5.0   \n",
            "3       0íšŒ      0íšŒ        0íšŒ       0íšŒ      0íšŒ        0íšŒ       0íšŒ        0.0   \n",
            "4       0íšŒ      0íšŒ        0íšŒ       0íšŒ      0íšŒ        0íšŒ       0íšŒ        6.0   \n",
            "\n",
            "   ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜  ë¯¸ì„¸ì£¼ìž…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜  ì´ì‹ëœ ë°°ì•„ ìˆ˜  ë¯¸ì„¸ì£¼ìž… ë°°ì•„ ì´ì‹ ìˆ˜  ì €ìž¥ëœ ë°°ì•„ ìˆ˜  \\\n",
            "0         5.0              4.0       2.0           2.0       2.0   \n",
            "1         1.0              0.0       0.0           0.0       0.0   \n",
            "2         0.0              0.0       2.0           0.0       0.0   \n",
            "3         4.0              0.0       0.0           0.0       0.0   \n",
            "4         6.0              6.0       2.0           2.0       0.0   \n",
            "\n",
            "   ë¯¸ì„¸ì£¼ìž… í›„ ì €ìž¥ëœ ë°°ì•„ ìˆ˜  í•´ë™ëœ ë°°ì•„ ìˆ˜  í•´ë™ ë‚œìž ìˆ˜  ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜  ì €ìž¥ëœ ì‹ ì„  ë‚œìž ìˆ˜  í˜¼í•©ëœ ë‚œìž ìˆ˜  \\\n",
            "0              2.0       0.0      0.0          7.0          0.0       5.0   \n",
            "1              0.0       0.0      0.0          1.0          0.0       1.0   \n",
            "2              0.0       0.0      0.0          8.0          0.0       7.0   \n",
            "3              0.0       0.0      0.0          5.0          0.0       4.0   \n",
            "4              0.0       0.0      0.0          7.0          0.0       6.0   \n",
            "\n",
            "   íŒŒíŠ¸ë„ˆ ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜  ê¸°ì¦ìž ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜  ë‚œìž ì¶œì²˜   ì •ìž ì¶œì²˜ ë‚œìž ê¸°ì¦ìž ë‚˜ì´ ì •ìž ê¸°ì¦ìž ë‚˜ì´  \\\n",
            "0               5.0               0.0  ë³¸ì¸ ì œê³µ  ë°°ìš°ìž ì œê³µ    ì•Œ ìˆ˜ ì—†ìŒ    ì•Œ ìˆ˜ ì—†ìŒ   \n",
            "1               1.0               0.0  ë³¸ì¸ ì œê³µ  ë°°ìš°ìž ì œê³µ    ì•Œ ìˆ˜ ì—†ìŒ    ì•Œ ìˆ˜ ì—†ìŒ   \n",
            "2               7.0               0.0  ë³¸ì¸ ì œê³µ  ë°°ìš°ìž ì œê³µ    ì•Œ ìˆ˜ ì—†ìŒ    ì•Œ ìˆ˜ ì—†ìŒ   \n",
            "3               4.0               0.0  ë³¸ì¸ ì œê³µ  ë°°ìš°ìž ì œê³µ    ì•Œ ìˆ˜ ì—†ìŒ    ì•Œ ìˆ˜ ì—†ìŒ   \n",
            "4               6.0               0.0  ë³¸ì¸ ì œê³µ  ë°°ìš°ìž ì œê³µ    ì•Œ ìˆ˜ ì—†ìŒ    ì•Œ ìˆ˜ ì—†ìŒ   \n",
            "\n",
            "   ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€  ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€  ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€  ëŒ€ë¦¬ëª¨ ì—¬ë¶€  PGD ì‹œìˆ  ì—¬ë¶€  PGS ì‹œìˆ  ì—¬ë¶€  \\\n",
            "0          0.0          1.0          0.0     0.0        NaN        NaN   \n",
            "1          0.0          1.0          0.0     0.0        NaN        NaN   \n",
            "2          0.0          1.0          0.0     0.0        NaN        NaN   \n",
            "3          0.0          1.0          0.0     0.0        NaN        NaN   \n",
            "4          0.0          1.0          0.0     0.0        NaN        NaN   \n",
            "\n",
            "   ë‚œìž ì±„ì·¨ ê²½ê³¼ì¼  ë‚œìž í•´ë™ ê²½ê³¼ì¼  ë‚œìž í˜¼í•© ê²½ê³¼ì¼  ë°°ì•„ ì´ì‹ ê²½ê³¼ì¼  ë°°ì•„ í•´ë™ ê²½ê³¼ì¼  ìž„ì‹  ì„±ê³µ ì—¬ë¶€  \n",
            "0        0.0        NaN        0.0        3.0        NaN         0  \n",
            "1        0.0        NaN        0.0        NaN        NaN         0  \n",
            "2        0.0        NaN        0.0        2.0        NaN         0  \n",
            "3        0.0        NaN        0.0        NaN        NaN         0  \n",
            "4        0.0        NaN        0.0        3.0        NaN         0  \n",
            "\n",
            "ðŸ“Š Train ë°ì´í„° ê¸°ìˆ í†µê³„ (ìˆ˜ì¹˜í˜• ì»¬ëŸ¼):\n",
            "       ìž„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ìž„ì‹  ê²½ê³¼ ì—°ìˆ˜       ë°°ëž€ ìžê·¹ ì—¬ë¶€    ë‹¨ì¼ ë°°ì•„ ì´ì‹ ì—¬ë¶€  ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€  \\\n",
            "count            9370.000000  256351.000000  250060.000000            2718.0   \n",
            "mean                9.270651       0.771286       0.233476               1.0   \n",
            "std                 3.550313       0.420005       0.423043               0.0   \n",
            "min                 0.000000       0.000000       0.000000               1.0   \n",
            "25%                 7.000000       1.000000       0.000000               1.0   \n",
            "50%                 9.000000       1.000000       0.000000               1.0   \n",
            "75%                11.000000       1.000000       0.000000               1.0   \n",
            "max                20.000000       1.000000       1.000000               1.0   \n",
            "\n",
            "       ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€     ë‚¨ì„± ì£¼ ë¶ˆìž„ ì›ì¸     ë‚¨ì„± ë¶€ ë¶ˆìž„ ì›ì¸     ì—¬ì„± ì£¼ ë¶ˆìž„ ì›ì¸  \\\n",
            "count     250060.000000  256351.000000  256351.000000  256351.000000   \n",
            "mean           0.012781       0.028516       0.013115       0.030724   \n",
            "std            0.112328       0.166441       0.113767       0.172568   \n",
            "min            0.000000       0.000000       0.000000       0.000000   \n",
            "25%            0.000000       0.000000       0.000000       0.000000   \n",
            "50%            0.000000       0.000000       0.000000       0.000000   \n",
            "75%            0.000000       0.000000       0.000000       0.000000   \n",
            "max            1.000000       1.000000       1.000000       1.000000   \n",
            "\n",
            "          ì—¬ì„± ë¶€ ë¶ˆìž„ ì›ì¸     ë¶€ë¶€ ì£¼ ë¶ˆìž„ ì›ì¸     ë¶€ë¶€ ë¶€ ë¶ˆìž„ ì›ì¸      ë¶ˆëª…í™• ë¶ˆìž„ ì›ì¸  \\\n",
            "count  256351.000000  256351.000000  256351.000000  256351.000000   \n",
            "mean        0.012432       0.033068       0.008765       0.250730   \n",
            "std         0.110805       0.178814       0.093212       0.433434   \n",
            "min         0.000000       0.000000       0.000000       0.000000   \n",
            "25%         0.000000       0.000000       0.000000       0.000000   \n",
            "50%         0.000000       0.000000       0.000000       0.000000   \n",
            "75%         0.000000       0.000000       0.000000       1.000000   \n",
            "max         1.000000       1.000000       1.000000       1.000000   \n",
            "\n",
            "       ë¶ˆìž„ ì›ì¸ - ë‚œê´€ ì§ˆí™˜  ë¶ˆìž„ ì›ì¸ - ë‚¨ì„± ìš”ì¸  ë¶ˆìž„ ì›ì¸ - ë°°ëž€ ìž¥ì•   ë¶ˆìž„ ì›ì¸ - ì—¬ì„± ìš”ì¸  \\\n",
            "count  256351.000000  256351.000000  256351.000000       256351.0   \n",
            "mean        0.138704       0.372403       0.130392            0.0   \n",
            "std         0.345638       0.483446       0.336734            0.0   \n",
            "min         0.000000       0.000000       0.000000            0.0   \n",
            "25%         0.000000       0.000000       0.000000            0.0   \n",
            "50%         0.000000       0.000000       0.000000            0.0   \n",
            "75%         0.000000       1.000000       0.000000            0.0   \n",
            "max         1.000000       1.000000       1.000000            0.0   \n",
            "\n",
            "       ë¶ˆìž„ ì›ì¸ - ìžê¶ê²½ë¶€ ë¬¸ì œ  ë¶ˆìž„ ì›ì¸ - ìžê¶ë‚´ë§‰ì¦  ë¶ˆìž„ ì›ì¸ - ì •ìž ë†ë„  ë¶ˆìž„ ì›ì¸ - ì •ìž ë©´ì—­í•™ì  ìš”ì¸  \\\n",
            "count    256351.000000  256351.000000  256351.000000       256351.000000   \n",
            "mean          0.000039       0.071394       0.001077            0.000004   \n",
            "std           0.006246       0.257483       0.032795            0.001975   \n",
            "min           0.000000       0.000000       0.000000            0.000000   \n",
            "25%           0.000000       0.000000       0.000000            0.000000   \n",
            "50%           0.000000       0.000000       0.000000            0.000000   \n",
            "75%           0.000000       0.000000       0.000000            0.000000   \n",
            "max           1.000000       1.000000       1.000000            1.000000   \n",
            "\n",
            "       ë¶ˆìž„ ì›ì¸ - ì •ìž ìš´ë™ì„±  ë¶ˆìž„ ì›ì¸ - ì •ìž í˜•íƒœ      ì´ ìƒì„± ë°°ì•„ ìˆ˜     ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜  \\\n",
            "count   256351.000000  256351.000000  250060.000000  250060.000000   \n",
            "mean         0.000378       0.000558       5.061145       4.170799   \n",
            "std          0.019449       0.023612       4.664337       5.625943   \n",
            "min          0.000000       0.000000       0.000000       0.000000   \n",
            "25%          0.000000       0.000000       1.000000       0.000000   \n",
            "50%          0.000000       0.000000       4.000000       0.000000   \n",
            "75%          0.000000       0.000000       8.000000       7.000000   \n",
            "max          1.000000       1.000000      51.000000      51.000000   \n",
            "\n",
            "       ë¯¸ì„¸ì£¼ìž…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜       ì´ì‹ëœ ë°°ì•„ ìˆ˜   ë¯¸ì„¸ì£¼ìž… ë°°ì•„ ì´ì‹ ìˆ˜       ì €ìž¥ëœ ë°°ì•„ ìˆ˜  \\\n",
            "count     250060.00000  250060.000000  250060.000000  250060.000000   \n",
            "mean           2.84365       1.368156       0.724718       1.185975   \n",
            "std            4.09464       0.771961       0.894352       2.502318   \n",
            "min            0.00000       0.000000       0.000000       0.000000   \n",
            "25%            0.00000       1.000000       0.000000       0.000000   \n",
            "50%            0.00000       1.000000       0.000000       0.000000   \n",
            "75%            5.00000       2.000000       2.000000       2.000000   \n",
            "max           43.00000       3.000000       3.000000      51.000000   \n",
            "\n",
            "       ë¯¸ì„¸ì£¼ìž… í›„ ì €ìž¥ëœ ë°°ì•„ ìˆ˜       í•´ë™ëœ ë°°ì•„ ìˆ˜        í•´ë™ ë‚œìž ìˆ˜    ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜  \\\n",
            "count    250060.000000  250060.000000  250060.000000  250060.000000   \n",
            "mean          0.612933       0.454571       0.046401       8.563717   \n",
            "std           1.842522       1.415320       0.688246       7.317230   \n",
            "min           0.000000       0.000000       0.000000       0.000000   \n",
            "25%           0.000000       0.000000       0.000000       2.000000   \n",
            "50%           0.000000       0.000000       0.000000       8.000000   \n",
            "75%           0.000000       0.000000       0.000000      13.000000   \n",
            "max          51.000000      32.000000      36.000000      51.000000   \n",
            "\n",
            "         ì €ìž¥ëœ ì‹ ì„  ë‚œìž ìˆ˜       í˜¼í•©ëœ ë‚œìž ìˆ˜  íŒŒíŠ¸ë„ˆ ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜  ê¸°ì¦ìž ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜  \\\n",
            "count  250060.000000  250060.000000     250060.000000     250060.000000   \n",
            "mean        0.093418       7.698928          7.107326          0.534728   \n",
            "std         1.105882       6.451914          6.532466          2.509487   \n",
            "min         0.000000       0.000000          0.000000          0.000000   \n",
            "25%         0.000000       3.000000          0.000000          0.000000   \n",
            "50%         0.000000       7.000000          6.000000          0.000000   \n",
            "75%         0.000000      11.000000         11.000000          0.000000   \n",
            "max        51.000000      51.000000         51.000000         50.000000   \n",
            "\n",
            "         ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€    ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€    ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€         ëŒ€ë¦¬ëª¨ ì—¬ë¶€  PGD ì‹œìˆ  ì—¬ë¶€  \\\n",
            "count  250060.000000  250060.000000  250060.000000  250060.000000     2179.0   \n",
            "mean        0.160465       0.840342       0.009830       0.004195        1.0   \n",
            "std         0.367038       0.366289       0.098656       0.064633        0.0   \n",
            "min         0.000000       0.000000       0.000000       0.000000        1.0   \n",
            "25%         0.000000       1.000000       0.000000       0.000000        1.0   \n",
            "50%         0.000000       1.000000       0.000000       0.000000        1.0   \n",
            "75%         0.000000       1.000000       0.000000       0.000000        1.0   \n",
            "max         1.000000       1.000000       1.000000       1.000000        1.0   \n",
            "\n",
            "       PGS ì‹œìˆ  ì—¬ë¶€  ë‚œìž ì±„ì·¨ ê²½ê³¼ì¼    ë‚œìž í•´ë™ ê²½ê³¼ì¼      ë‚œìž í˜¼í•© ê²½ê³¼ì¼      ë°°ì•„ ì´ì‹ ê²½ê³¼ì¼  \\\n",
            "count     1929.0   198863.0  1436.000000  202616.000000  212785.000000   \n",
            "mean         1.0        0.0     0.001393       0.005385       3.254741   \n",
            "std          0.0        0.0     0.037307       0.111504       1.715697   \n",
            "min          1.0        0.0     0.000000       0.000000       0.000000   \n",
            "25%          1.0        0.0     0.000000       0.000000       2.000000   \n",
            "50%          1.0        0.0     0.000000       0.000000       3.000000   \n",
            "75%          1.0        0.0     0.000000       0.000000       5.000000   \n",
            "max          1.0        0.0     1.000000       7.000000       7.000000   \n",
            "\n",
            "          ë°°ì•„ í•´ë™ ê²½ê³¼ì¼  \n",
            "count  40369.000000  \n",
            "mean       0.045629  \n",
            "std        0.418672  \n",
            "min        0.000000  \n",
            "25%        0.000000  \n",
            "50%        0.000000  \n",
            "75%        0.000000  \n",
            "max        7.000000  \n",
            "\n",
            "======================================================================\n",
            "âœ… 2ë‹¨ê³„ ì™„ë£Œ: ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ë¶„ì„\n",
            "ðŸ”„ ë‹¤ìŒ: ë°ì´í„° íƒ€ìž…ë³„ ìƒì„¸ ë¶„ì„ ë° ì‹œê°í™”\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# ìž„ì‹  ì„±ê³µ ì—¬ë¶€ ì˜ˆì¸¡ ëª¨ë¸ - ì„±ëŠ¥ í–¥ìƒ ë²„ì „\n",
        "# 2ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA)\n",
        "# ====================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ“‚ 2ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì‹œìž‘\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. ë°ì´í„° ë¡œë“œ\n",
        "print(\"ðŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "\n",
        "print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"   ðŸ“ˆ Train ë°ì´í„°: {train.shape}\")\n",
        "print(f\"   ðŸ“‰ Test ë°ì´í„°: {test.shape}\")\n",
        "\n",
        "# 2. ê¸°ë³¸ ë°ì´í„° ì •ë³´\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ“‹ ê¸°ë³¸ ë°ì´í„° ì •ë³´\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\nðŸ” Train ë°ì´í„° ê¸°ë³¸ ì •ë³´:\")\n",
        "print(f\"   - í–‰ ìˆ˜: {train.shape[0]:,}\")\n",
        "print(f\"   - ì—´ ìˆ˜: {train.shape[1]:,}\")\n",
        "print(f\"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "print(\"\\nðŸ” Test ë°ì´í„° ê¸°ë³¸ ì •ë³´:\")\n",
        "print(f\"   - í–‰ ìˆ˜: {test.shape[0]:,}\")\n",
        "print(f\"   - ì—´ ìˆ˜: {test.shape[1]:,}\")\n",
        "print(f\"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {test.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# 3. ì»¬ëŸ¼ ì •ë³´\n",
        "print(f\"\\nðŸ“ ì „ì²´ ì»¬ëŸ¼ ëª©ë¡ ({len(train.columns)}ê°œ):\")\n",
        "for i, col in enumerate(train.columns, 1):\n",
        "    print(f\"   {i:2d}. {col}\")\n",
        "\n",
        "# 4. ë°ì´í„° íƒ€ìž… í™•ì¸\n",
        "print(f\"\\nðŸ·ï¸  ë°ì´í„° íƒ€ìž… ë¶„í¬:\")\n",
        "train_dtypes = train.dtypes.value_counts()\n",
        "for dtype, count in train_dtypes.items():\n",
        "    print(f\"   {dtype}: {count}ê°œ\")\n",
        "\n",
        "# 5. ID ì»¬ëŸ¼ í™•ì¸ ë° ì œê±°\n",
        "if 'ID' in train.columns:\n",
        "    print(f\"\\nðŸ”‘ ID ì»¬ëŸ¼ ë°œê²¬ - ì œê±° ì§„í–‰\")\n",
        "    train_ids = train['ID'].copy()\n",
        "    test_ids = test['ID'].copy()\n",
        "    train = train.drop('ID', axis=1)\n",
        "    test = test.drop('ID', axis=1)\n",
        "    print(f\"   âœ… ID ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\")\n",
        "    print(f\"   ðŸ“ˆ Train: {train.shape}, Test: {test.shape}\")\n",
        "\n",
        "# 6. íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬\n",
        "target_col = 'ìž„ì‹  ì„±ê³µ ì—¬ë¶€'\n",
        "if target_col in train.columns:\n",
        "    X = train.drop(target_col, axis=1)\n",
        "    y = train[target_col]\n",
        "    print(f\"\\nðŸŽ¯ íƒ€ê²Ÿ ë³€ìˆ˜ '{target_col}' ë¶„ë¦¬ ì™„ë£Œ\")\n",
        "    print(f\"   ðŸ“Š íŠ¹ì„± ë°ì´í„°: {X.shape}\")\n",
        "    print(f\"   ðŸŽ¯ íƒ€ê²Ÿ ë°ì´í„°: {y.shape}\")\n",
        "else:\n",
        "    print(f\"\\nâŒ íƒ€ê²Ÿ ë³€ìˆ˜ '{target_col}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "\n",
        "# 7. íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ ë¶„ì„\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸŽ¯ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ ë¶„ì„\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "target_counts = y.value_counts().sort_index()\n",
        "target_props = y.value_counts(normalize=True).sort_index()\n",
        "\n",
        "print(\"ðŸ“Š í´ëž˜ìŠ¤ ë¶„í¬:\")\n",
        "for class_val in sorted(y.unique()):\n",
        "    count = target_counts[class_val]\n",
        "    prop = target_props[class_val]\n",
        "    print(f\"   í´ëž˜ìŠ¤ {class_val}: {count:,}ê°œ ({prop:.2%})\")\n",
        "\n",
        "# ë¶ˆê· í˜• ì •ë„ ê³„ì‚°\n",
        "imbalance_ratio = target_counts.max() / target_counts.min()\n",
        "print(f\"\\nâš–ï¸  í´ëž˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "if imbalance_ratio > 2:\n",
        "    print(\"   âš ï¸  ë¶ˆê· í˜• ë°ì´í„° - SMOTE ë“± ìƒ˜í”Œë§ ê¸°ë²• ê³ ë ¤ í•„ìš”\")\n",
        "else:\n",
        "    print(\"   âœ… ë¹„êµì  ê· í˜•ìž¡ížŒ ë°ì´í„°\")\n",
        "\n",
        "# 8. ê²°ì¸¡ì¹˜ ë¶„ì„\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ” ê²°ì¸¡ì¹˜ ë¶„ì„\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Train ë°ì´í„° ê²°ì¸¡ì¹˜\n",
        "print(\"ðŸ“ˆ Train ë°ì´í„° ê²°ì¸¡ì¹˜:\")\n",
        "train_missing = X.isnull().sum()\n",
        "train_missing_pct = (train_missing / len(X)) * 100\n",
        "missing_info = pd.DataFrame({\n",
        "    'ê²°ì¸¡ì¹˜_ê°œìˆ˜': train_missing,\n",
        "    'ê²°ì¸¡ì¹˜_ë¹„ìœ¨(%)': train_missing_pct\n",
        "}).sort_values('ê²°ì¸¡ì¹˜_ê°œìˆ˜', ascending=False)\n",
        "\n",
        "# ê²°ì¸¡ì¹˜ê°€ ìžˆëŠ” ì»¬ëŸ¼ë§Œ í‘œì‹œ\n",
        "missing_cols = missing_info[missing_info['ê²°ì¸¡ì¹˜_ê°œìˆ˜'] > 0]\n",
        "if len(missing_cols) > 0:\n",
        "    print(f\"   ê²°ì¸¡ì¹˜ê°€ ìžˆëŠ” ì»¬ëŸ¼: {len(missing_cols)}ê°œ\")\n",
        "    print(missing_cols.head(10))\n",
        "else:\n",
        "    print(\"   âœ… ê²°ì¸¡ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "\n",
        "# Test ë°ì´í„° ê²°ì¸¡ì¹˜\n",
        "print(f\"\\nðŸ“‰ Test ë°ì´í„° ê²°ì¸¡ì¹˜:\")\n",
        "test_missing = test.isnull().sum()\n",
        "test_missing_pct = (test_missing / len(test)) * 100\n",
        "test_missing_info = pd.DataFrame({\n",
        "    'ê²°ì¸¡ì¹˜_ê°œìˆ˜': test_missing,\n",
        "    'ê²°ì¸¡ì¹˜_ë¹„ìœ¨(%)': test_missing_pct\n",
        "}).sort_values('ê²°ì¸¡ì¹˜_ê°œìˆ˜', ascending=False)\n",
        "\n",
        "test_missing_cols = test_missing_info[test_missing_info['ê²°ì¸¡ì¹˜_ê°œìˆ˜'] > 0]\n",
        "if len(test_missing_cols) > 0:\n",
        "    print(f\"   ê²°ì¸¡ì¹˜ê°€ ìžˆëŠ” ì»¬ëŸ¼: {len(test_missing_cols)}ê°œ\")\n",
        "    print(test_missing_cols.head(10))\n",
        "else:\n",
        "    print(\"   âœ… ê²°ì¸¡ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "\n",
        "# 9. ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ‘€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"ðŸ“Š Train ë°ì´í„° ìƒìœ„ 5í–‰:\")\n",
        "print(train.head())\n",
        "\n",
        "print(f\"\\nðŸ“Š Train ë°ì´í„° ê¸°ìˆ í†µê³„ (ìˆ˜ì¹˜í˜• ì»¬ëŸ¼):\")\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "if len(numeric_cols) > 0:\n",
        "    print(X[numeric_cols].describe())\n",
        "else:\n",
        "    print(\"   ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… 2ë‹¨ê³„ ì™„ë£Œ: ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ë¶„ì„\")\n",
        "print(\"ðŸ”„ ë‹¤ìŒ: ë°ì´í„° íƒ€ìž…ë³„ ìƒì„¸ ë¶„ì„ ë° ì‹œê°í™”\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3ë‹¨ê³„: ìƒì„¸ EDA ë° ì‹œê°í™”"
      ],
      "metadata": {
        "id": "QaPtYPt0zOzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# ìž„ì‹  ì„±ê³µ ì—¬ë¶€ ì˜ˆì¸¡ ëª¨ë¸ - ì„±ëŠ¥ í–¥ìƒ ë²„ì „\n",
        "# 3ë‹¨ê³„: ìƒì„¸ EDA ë° ì‹œê°í™”\n",
        "# ====================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ“Š 3ë‹¨ê³„: ìƒì„¸ EDA ë° ì‹œê°í™” ë¶„ì„ ì‹œìž‘\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ë°ì´í„° ëª…ì„¸ì—ì„œ íŒŒì•…í•œ íŠ¹ì„± ë¶„ë¥˜\n",
        "categorical_features = [\n",
        "    \"ì‹œìˆ  ì‹œê¸° ì½”ë“œ\", \"ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´\", \"ì‹œìˆ  ìœ í˜•\", \"ë°°ëž€ ìžê·¹ ì—¬ë¶€\", \"ë°°ëž€ ìœ ë„ ìœ í˜•\",\n",
        "    \"ë‹¨ì¼ ë°°ì•„ ì´ì‹ ì—¬ë¶€\", \"ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€\", \"ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€\",\n",
        "    \"ë‚¨ì„± ì£¼ ë¶ˆìž„ ì›ì¸\", \"ë‚¨ì„± ë¶€ ë¶ˆìž„ ì›ì¸\", \"ì—¬ì„± ì£¼ ë¶ˆìž„ ì›ì¸\", \"ì—¬ì„± ë¶€ ë¶ˆìž„ ì›ì¸\",\n",
        "    \"ë¶€ë¶€ ì£¼ ë¶ˆìž„ ì›ì¸\", \"ë¶€ë¶€ ë¶€ ë¶ˆìž„ ì›ì¸\", \"ë¶ˆëª…í™• ë¶ˆìž„ ì›ì¸\", \"ë¶ˆìž„ ì›ì¸ - ë‚œê´€ ì§ˆí™˜\",\n",
        "    \"ë¶ˆìž„ ì›ì¸ - ë‚¨ì„± ìš”ì¸\", \"ë¶ˆìž„ ì›ì¸ - ë°°ëž€ ìž¥ì• \", \"ë¶ˆìž„ ì›ì¸ - ì—¬ì„± ìš”ì¸\",\n",
        "    \"ë¶ˆìž„ ì›ì¸ - ìžê¶ê²½ë¶€ ë¬¸ì œ\", \"ë¶ˆìž„ ì›ì¸ - ìžê¶ë‚´ë§‰ì¦\", \"ë¶ˆìž„ ì›ì¸ - ì •ìž ë†ë„\",\n",
        "    \"ë¶ˆìž„ ì›ì¸ - ì •ìž ë©´ì—­í•™ì  ìš”ì¸\", \"ë¶ˆìž„ ì›ì¸ - ì •ìž ìš´ë™ì„±\", \"ë¶ˆìž„ ì›ì¸ - ì •ìž í˜•íƒœ\",\n",
        "    \"ë°°ì•„ ìƒì„± ì£¼ìš” ì´ìœ \", \"ì´ ì‹œìˆ  íšŸìˆ˜\", \"í´ë¦¬ë‹‰ ë‚´ ì´ ì‹œìˆ  íšŸìˆ˜\", \"IVF ì‹œìˆ  íšŸìˆ˜\",\n",
        "    \"DI ì‹œìˆ  íšŸìˆ˜\", \"ì´ ìž„ì‹  íšŸìˆ˜\", \"IVF ìž„ì‹  íšŸìˆ˜\", \"DI ìž„ì‹  íšŸìˆ˜\", \"ì´ ì¶œì‚° íšŸìˆ˜\",\n",
        "    \"IVF ì¶œì‚° íšŸìˆ˜\", \"DI ì¶œì‚° íšŸìˆ˜\", \"ë‚œìž ì¶œì²˜\", \"ì •ìž ì¶œì²˜\", \"ë‚œìž ê¸°ì¦ìž ë‚˜ì´\",\n",
        "    \"ì •ìž ê¸°ì¦ìž ë‚˜ì´\", \"ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\", \"ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\", \"ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\",\n",
        "    \"ëŒ€ë¦¬ëª¨ ì—¬ë¶€\", \"PGD ì‹œìˆ  ì—¬ë¶€\", \"PGS ì‹œìˆ  ì—¬ë¶€\"\n",
        "]\n",
        "\n",
        "numerical_features = [\n",
        "    \"ìž„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ìž„ì‹  ê²½ê³¼ ì—°ìˆ˜\", \"íŠ¹ì • ì‹œìˆ  ìœ í˜•\", \"ì´ ìƒì„± ë°°ì•„ ìˆ˜\",\n",
        "    \"ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜\", \"ë¯¸ì„¸ì£¼ìž…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜\", \"ì´ì‹ëœ ë°°ì•„ ìˆ˜\",\n",
        "    \"ë¯¸ì„¸ì£¼ìž… ë°°ì•„ ì´ì‹ ìˆ˜\", \"ì €ìž¥ëœ ë°°ì•„ ìˆ˜\", \"ë¯¸ì„¸ì£¼ìž… í›„ ì €ìž¥ëœ ë°°ì•„ ìˆ˜\",\n",
        "    \"í•´ë™ëœ ë°°ì•„ ìˆ˜\", \"í•´ë™ ë‚œìž ìˆ˜\", \"ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜\", \"ì €ìž¥ëœ ì‹ ì„  ë‚œìž ìˆ˜\",\n",
        "    \"í˜¼í•©ëœ ë‚œìž ìˆ˜\", \"íŒŒíŠ¸ë„ˆ ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜\", \"ê¸°ì¦ìž ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜\",\n",
        "    \"ë‚œìž ì±„ì·¨ ê²½ê³¼ì¼\", \"ë‚œìž í•´ë™ ê²½ê³¼ì¼\", \"ë‚œìž í˜¼í•© ê²½ê³¼ì¼\", \"ë°°ì•„ ì´ì‹ ê²½ê³¼ì¼\", \"ë°°ì•„ í•´ë™ ê²½ê³¼ì¼\"\n",
        "]\n",
        "\n",
        "# 1. ê²°ì¸¡ì¹˜ íŒ¨í„´ ìƒì„¸ ë¶„ì„\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ” ê²°ì¸¡ì¹˜ íŒ¨í„´ ìƒì„¸ ë¶„ì„\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ê²°ì¸¡ì¹˜ ë¹„ìœ¨ë³„ ë³€ìˆ˜ ë¶„ë¥˜\n",
        "missing_analysis = pd.DataFrame({\n",
        "    'column': X.columns,\n",
        "    'missing_count': X.isnull().sum(),\n",
        "    'missing_pct': (X.isnull().sum() / len(X)) * 100\n",
        "}).sort_values('missing_pct', ascending=False)\n",
        "\n",
        "# ê²°ì¸¡ì¹˜ íŒ¨í„´ë³„ ë¶„ë¥˜\n",
        "high_missing = missing_analysis[missing_analysis['missing_pct'] > 80]\n",
        "medium_missing = missing_analysis[(missing_analysis['missing_pct'] > 20) & (missing_analysis['missing_pct'] <= 80)]\n",
        "low_missing = missing_analysis[(missing_analysis['missing_pct'] > 0) & (missing_analysis['missing_pct'] <= 20)]\n",
        "no_missing = missing_analysis[missing_analysis['missing_pct'] == 0]\n",
        "\n",
        "print(f\"ðŸ“Š ê²°ì¸¡ì¹˜ íŒ¨í„´ë³„ ë³€ìˆ˜ ë¶„ë¥˜:\")\n",
        "print(f\"   ðŸ”´ ê³ ê²°ì¸¡ (80%+): {len(high_missing)}ê°œ ë³€ìˆ˜\")\n",
        "print(f\"   ðŸŸ¡ ì¤‘ê²°ì¸¡ (20-80%): {len(medium_missing)}ê°œ ë³€ìˆ˜\")\n",
        "print(f\"   ðŸŸ¢ ì €ê²°ì¸¡ (0-20%): {len(low_missing)}ê°œ ë³€ìˆ˜\")\n",
        "print(f\"   âœ… ê²°ì¸¡ ì—†ìŒ: {len(no_missing)}ê°œ ë³€ìˆ˜\")\n",
        "\n",
        "print(f\"\\nðŸ”´ ê³ ê²°ì¸¡ ë³€ìˆ˜ë“¤ (ì œê±° ê³ ë ¤):\")\n",
        "for _, row in high_missing.iterrows():\n",
        "    print(f\"   - {row['column']}: {row['missing_pct']:.1f}%\")\n",
        "\n",
        "print(f\"\\nðŸŸ¡ ì¤‘ê²°ì¸¡ ë³€ìˆ˜ë“¤ (íŠ¹ë³„ ì²˜ë¦¬ í•„ìš”):\")\n",
        "for _, row in medium_missing.iterrows():\n",
        "    print(f\"   - {row['column']}: {row['missing_pct']:.1f}%\")\n",
        "\n",
        "# 2. íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ê´€ê³„ ë¶„ì„ (ë²”ì£¼í˜• ë³€ìˆ˜)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸŽ¯ ë²”ì£¼í˜• ë³€ìˆ˜ì™€ íƒ€ê²Ÿì˜ ê´€ê³„ ë¶„ì„\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ê²°ì¸¡ì¹˜ê°€ ì ì€ ì¤‘ìš”í•œ ë²”ì£¼í˜• ë³€ìˆ˜ë“¤ ì„ ë³„\n",
        "important_categorical = [\n",
        "    \"ì‹œìˆ  ì‹œê¸° ì½”ë“œ\", \"ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´\", \"ì‹œìˆ  ìœ í˜•\", \"ë°°ëž€ ìžê·¹ ì—¬ë¶€\",\n",
        "    \"ë°°ëž€ ìœ ë„ ìœ í˜•\", \"ë¶ˆìž„ ì›ì¸ - ë‚¨ì„± ìš”ì¸\", \"ë¶ˆìž„ ì›ì¸ - ë°°ëž€ ìž¥ì• \",\n",
        "    \"ë¶ˆìž„ ì›ì¸ - ë‚œê´€ ì§ˆí™˜\", \"ë¶ˆìž„ ì›ì¸ - ìžê¶ë‚´ë§‰ì¦\", \"ë°°ì•„ ìƒì„± ì£¼ìš” ì´ìœ \",\n",
        "    \"ë‚œìž ì¶œì²˜\", \"ì •ìž ì¶œì²˜\"\n",
        "]\n",
        "\n",
        "target_categorical_analysis = {}\n",
        "for col in important_categorical:\n",
        "    if col in X.columns:\n",
        "        # íƒ€ê²Ÿë³„ ë¶„í¬ ê³„ì‚°\n",
        "        crosstab = pd.crosstab(X[col], y, normalize='index')\n",
        "        success_rate = crosstab[1] if 1 in crosstab.columns else pd.Series()\n",
        "        target_categorical_analysis[col] = {\n",
        "            'success_rates': success_rate,\n",
        "            'unique_count': X[col].nunique(),\n",
        "            'most_common': X[col].value_counts().head(3)\n",
        "        }\n",
        "\n",
        "print(\"ðŸ“Š ì£¼ìš” ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ìž„ì‹  ì„±ê³µë¥ :\")\n",
        "for col, analysis in target_categorical_analysis.items():\n",
        "    print(f\"\\nðŸ”¸ {col}:\")\n",
        "    print(f\"   ì¹´í…Œê³ ë¦¬ ìˆ˜: {analysis['unique_count']}ê°œ\")\n",
        "    if len(analysis['success_rates']) > 0:\n",
        "        print(f\"   ìµœê³  ì„±ê³µë¥ : {analysis['success_rates'].max():.3f}\")\n",
        "        print(f\"   ìµœì € ì„±ê³µë¥ : {analysis['success_rates'].min():.3f}\")\n",
        "        print(f\"   ì„±ê³µë¥  íŽ¸ì°¨: {analysis['success_rates'].std():.3f}\")\n",
        "\n",
        "# 3. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ ë¶„ì„\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ“ˆ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ ë¶„ì„\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ê²°ì¸¡ì¹˜ê°€ ì ì€ ì¤‘ìš”í•œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤\n",
        "important_numerical = [\n",
        "    \"ì´ ìƒì„± ë°°ì•„ ìˆ˜\", \"ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜\", \"ë¯¸ì„¸ì£¼ìž…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜\",\n",
        "    \"ì´ì‹ëœ ë°°ì•„ ìˆ˜\", \"ë¯¸ì„¸ì£¼ìž… ë°°ì•„ ì´ì‹ ìˆ˜\", \"ì €ìž¥ëœ ë°°ì•„ ìˆ˜\",\n",
        "    \"ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜\", \"í˜¼í•©ëœ ë‚œìž ìˆ˜\", \"íŒŒíŠ¸ë„ˆ ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜\"\n",
        "]\n",
        "\n",
        "print(\"ðŸ“Š ì£¼ìš” ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í†µê³„:\")\n",
        "for col in important_numerical:\n",
        "    if col in X.columns:\n",
        "        col_data = X[col].dropna()\n",
        "        if len(col_data) > 0:\n",
        "            print(f\"\\nðŸ”¸ {col}:\")\n",
        "            print(f\"   í‰ê· : {col_data.mean():.2f}\")\n",
        "            print(f\"   ì¤‘ìœ„ìˆ˜: {col_data.median():.2f}\")\n",
        "            print(f\"   í‘œì¤€íŽ¸ì°¨: {col_data.std():.2f}\")\n",
        "            print(f\"   ìµœëŒ“ê°’: {col_data.max():.0f}\")\n",
        "            print(f\"   0ì¸ ë¹„ìœ¨: {(col_data == 0).mean():.2%}\")\n",
        "\n",
        "            # íƒ€ê²Ÿë³„ í‰ê·  ë¹„êµ\n",
        "            target_0_mean = X.loc[y == 0, col].mean()\n",
        "            target_1_mean = X.loc[y == 1, col].mean()\n",
        "            print(f\"   ì‹¤íŒ¨êµ° í‰ê· : {target_0_mean:.2f}\")\n",
        "            print(f\"   ì„±ê³µêµ° í‰ê· : {target_1_mean:.2f}\")\n",
        "            print(f\"   ì°¨ì´: {target_1_mean - target_0_mean:.2f}\")\n",
        "\n",
        "# 4. ìƒê´€ê´€ê³„ ë¶„ì„\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ”— ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„\n",
        "numeric_data = X[important_numerical].select_dtypes(include=[np.number])\n",
        "correlation_matrix = numeric_data.corr()\n",
        "\n",
        "# ë†’ì€ ìƒê´€ê´€ê³„ ì°¾ê¸°\n",
        "high_corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_value = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr_value) > 0.7:  # 0.7 ì´ìƒì˜ ìƒê´€ê´€ê³„\n",
        "            high_corr_pairs.append({\n",
        "                'var1': correlation_matrix.columns[i],\n",
        "                'var2': correlation_matrix.columns[j],\n",
        "                'correlation': corr_value\n",
        "            })\n",
        "\n",
        "print(f\"ðŸ“Š ë†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.7) ë³€ìˆ˜ ìŒ: {len(high_corr_pairs)}ê°œ\")\n",
        "for pair in high_corr_pairs:\n",
        "    print(f\"   {pair['var1']} â†” {pair['var2']}: {pair['correlation']:.3f}\")\n",
        "\n",
        "# 5. ì´ìƒì¹˜ íƒì§€\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ“Š ì´ìƒì¹˜ íƒì§€ (IQR ë°©ë²•)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "outlier_summary = {}\n",
        "for col in important_numerical:\n",
        "    if col in X.columns:\n",
        "        col_data = X[col].dropna()\n",
        "        if len(col_data) > 0:\n",
        "            Q1 = col_data.quantile(0.25)\n",
        "            Q3 = col_data.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]\n",
        "            outlier_pct = len(outliers) / len(col_data) * 100\n",
        "\n",
        "            outlier_summary[col] = {\n",
        "                'count': len(outliers),\n",
        "                'percentage': outlier_pct,\n",
        "                'bounds': (lower_bound, upper_bound)\n",
        "            }\n",
        "\n",
        "print(\"ðŸ“Š ì´ìƒì¹˜ í˜„í™©:\")\n",
        "for col, info in outlier_summary.items():\n",
        "    if info['percentage'] > 5:  # 5% ì´ìƒì¸ ê²½ìš°ë§Œ í‘œì‹œ\n",
        "        print(f\"   {col}: {info['count']}ê°œ ({info['percentage']:.1f}%)\")\n",
        "\n",
        "# 6. íŠ¹ì„± ì¤‘ìš”ë„ (ìƒí˜¸ì •ë³´ëŸ‰)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸŽ¯ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ìƒí˜¸ì •ë³´ëŸ‰)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# ê²°ì¸¡ì¹˜ê°€ ì ì€ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ë¡œ ìƒí˜¸ì •ë³´ëŸ‰ ê³„ì‚°\n",
        "clean_numeric_cols = []\n",
        "X_clean_numeric = pd.DataFrame()\n",
        "\n",
        "for col in important_numerical:\n",
        "    if col in X.columns:\n",
        "        col_data = X[col].fillna(X[col].median())  # ìž„ì‹œë¡œ ì¤‘ìœ„ìˆ˜ë¡œ ê²°ì¸¡ì¹˜ ì±„ì›€\n",
        "        if col_data.nunique() > 1:  # ìƒìˆ˜ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ\n",
        "            X_clean_numeric[col] = col_data\n",
        "            clean_numeric_cols.append(col)\n",
        "\n",
        "if len(X_clean_numeric) > 0:\n",
        "    mi_scores = mutual_info_classif(X_clean_numeric, y, random_state=42)\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': clean_numeric_cols,\n",
        "        'importance': mi_scores\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(\"ðŸ“Š ìƒí˜¸ì •ë³´ëŸ‰ ê¸°ë°˜ íŠ¹ì„± ì¤‘ìš”ë„ (Top 10):\")\n",
        "    for _, row in feature_importance.head(10).iterrows():\n",
        "        print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… 3ë‹¨ê³„ ì™„ë£Œ: ìƒì„¸ EDA ë° ë¶„ì„\")\n",
        "print(\"ðŸ”„ ë‹¤ìŒ ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# EDA ìš”ì•½ ì •ë³´ ì €ìž¥\n",
        "eda_summary = {\n",
        "    'total_samples': len(X),\n",
        "    'total_features': len(X.columns),\n",
        "    'target_success_rate': y.mean(),\n",
        "    'imbalance_ratio': (y == 0).sum() / (y == 1).sum(),\n",
        "    'high_missing_features': len(high_missing),\n",
        "    'medium_missing_features': len(medium_missing),\n",
        "    'low_missing_features': len(low_missing),\n",
        "    'no_missing_features': len(no_missing),\n",
        "    'high_correlation_pairs': len(high_corr_pairs)\n",
        "}\n",
        "\n",
        "print(f\"\\nðŸ“‹ EDA ìš”ì•½:\")\n",
        "for key, value in eda_summary.items():\n",
        "    print(f\"   {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OLbVpxkzQCz",
        "outputId": "518189df-d56f-4e35-9dfe-199e9fdb4cdd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ðŸ“Š 3ë‹¨ê³„: ìƒì„¸ EDA ë° ì‹œê°í™” ë¶„ì„ ì‹œìž‘\n",
            "======================================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ” ê²°ì¸¡ì¹˜ íŒ¨í„´ ìƒì„¸ ë¶„ì„\n",
            "==================================================\n",
            "ðŸ“Š ê²°ì¸¡ì¹˜ íŒ¨í„´ë³„ ë³€ìˆ˜ ë¶„ë¥˜:\n",
            "   ðŸ”´ ê³ ê²°ì¸¡ (80%+): 6ê°œ ë³€ìˆ˜\n",
            "   ðŸŸ¡ ì¤‘ê²°ì¸¡ (20-80%): 2ê°œ ë³€ìˆ˜\n",
            "   ðŸŸ¢ ì €ê²°ì¸¡ (0-20%): 23ê°œ ë³€ìˆ˜\n",
            "   âœ… ê²°ì¸¡ ì—†ìŒ: 36ê°œ ë³€ìˆ˜\n",
            "\n",
            "ðŸ”´ ê³ ê²°ì¸¡ ë³€ìˆ˜ë“¤ (ì œê±° ê³ ë ¤):\n",
            "   - ë‚œìž í•´ë™ ê²½ê³¼ì¼: 99.4%\n",
            "   - PGS ì‹œìˆ  ì—¬ë¶€: 99.2%\n",
            "   - PGD ì‹œìˆ  ì—¬ë¶€: 99.1%\n",
            "   - ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€: 98.9%\n",
            "   - ìž„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ìž„ì‹  ê²½ê³¼ ì—°ìˆ˜: 96.3%\n",
            "   - ë°°ì•„ í•´ë™ ê²½ê³¼ì¼: 84.3%\n",
            "\n",
            "ðŸŸ¡ ì¤‘ê²°ì¸¡ ë³€ìˆ˜ë“¤ (íŠ¹ë³„ ì²˜ë¦¬ í•„ìš”):\n",
            "   - ë‚œìž ì±„ì·¨ ê²½ê³¼ì¼: 22.4%\n",
            "   - ë‚œìž í˜¼í•© ê²½ê³¼ì¼: 21.0%\n",
            "\n",
            "==================================================\n",
            "ðŸŽ¯ ë²”ì£¼í˜• ë³€ìˆ˜ì™€ íƒ€ê²Ÿì˜ ê´€ê³„ ë¶„ì„\n",
            "==================================================\n",
            "ðŸ“Š ì£¼ìš” ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ìž„ì‹  ì„±ê³µë¥ :\n",
            "\n",
            "ðŸ”¸ ì‹œìˆ  ì‹œê¸° ì½”ë“œ:\n",
            "   ì¹´í…Œê³ ë¦¬ ìˆ˜: 7ê°œ\n",
            "   ìµœê³  ì„±ê³µë¥ : 0.269\n",
            "   ìµœì € ì„±ê³µë¥ : 0.245\n",
            "   ì„±ê³µë¥  íŽ¸ì°¨: 0.008\n",
            "\n",
            "ðŸ”¸ ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´:\n",
            "   ì¹´í…Œê³ ë¦¬ ìˆ˜: 7ê°œ\n",
            "   ìµœê³  ì„±ê³µë¥ : 0.323\n",
            "   ìµœì € ì„±ê³µë¥ : 0.000\n",
            "   ì„±ê³µë¥  íŽ¸ì°¨: 0.107\n",
            "\n",
            "ðŸ”¸ ì‹œìˆ  ìœ í˜•:\n",
            "   ì¹´í…Œê³ ë¦¬ ìˆ˜: 2ê°œ\n",
            "   ìµœê³  ì„±ê³µë¥ : 0.262\n",
            "   ìµœì € ì„±ê³µë¥ : 0.129\n",
            "   ì„±ê³µë¥  íŽ¸ì°¨: 0.094\n",
            "\n",
            "ðŸ”¸ ë°°ëž€ ìžê·¹ ì—¬ë¶€:\n",
            "   ì¹´í…Œê³ ë¦¬ ìˆ˜: 2ê°œ\n",
            "   ìµœê³  ì„±ê³µë¥ : 0.266\n",
            "   ìµœì € ì„±ê³µë¥ : 0.232\n",
            "   ì„±ê³µë¥  íŽ¸ì°¨: 0.024\n",
            "\n",
            "ðŸ”¸ ë°°ëž€ ìœ ë„ ìœ í˜•:\n",
            "   ì¹´í…Œê³ ë¦¬ ìˆ˜: 4ê°œ\n",
            "   ìµœê³  ì„±ê³µë¥ : 1.000\n",
            "   ìµœì € ì„±ê³µë¥ : 0.000\n",
            "   ì„±ê³µë¥  íŽ¸ì°¨: 0.434\n",
            "\n",
            "ðŸ”¸ ë¶ˆìž„ ì›ì¸ - ë‚¨ì„± ìš”ì¸:\n",
            "   ì¹´í…Œê³ ë¦¬ ìˆ˜: 2ê°œ\n",
            "   ìµœê³  ì„±ê³µë¥ : 0.280\n",
            "   ìµœì € ì„±ê³µë¥ : 0.246\n",
            "   ì„±ê³µë¥  íŽ¸ì°¨: 0.024\n",
            "\n",
            "ðŸ”¸ ë¶ˆìž„ ì›ì¸ - ë°°ëž€ ìž¥ì• :\n",
            "   ì¹´í…Œê³ ë¦¬ ìˆ˜: 2ê°œ\n",
            "   ìµœê³  ì„±ê³µë¥ : 0.287\n",
            "   ìµœì € ì„±ê³µë¥ : 0.254\n",
            "   ì„±ê³µë¥  íŽ¸ì°¨: 0.023\n",
            "\n",
            "ðŸ”¸ ë¶ˆìž„ ì›ì¸ - ë‚œê´€ ì§ˆí™˜:\n",
            "   ì¹´í…Œê³ ë¦¬ ìˆ˜: 2ê°œ\n",
            "   ìµœê³  ì„±ê³µë¥ : 0.258\n",
            "   ìµœì € ì„±ê³µë¥ : 0.258\n",
            "   ì„±ê³µë¥  íŽ¸ì°¨: 0.000\n",
            "\n",
            "ðŸ”¸ ë¶ˆìž„ ì›ì¸ - ìžê¶ë‚´ë§‰ì¦:\n",
            "   ì¹´í…Œê³ ë¦¬ ìˆ˜: 2ê°œ\n",
            "   ìµœê³  ì„±ê³µë¥ : 0.259\n",
            "   ìµœì € ì„±ê³µë¥ : 0.255\n",
            "   ì„±ê³µë¥  íŽ¸ì°¨: 0.003\n",
            "\n",
            "ðŸ”¸ ë°°ì•„ ìƒì„± ì£¼ìš” ì´ìœ :\n",
            "   ì¹´í…Œê³ ë¦¬ ìˆ˜: 13ê°œ\n",
            "   ìµœê³  ì„±ê³µë¥ : 0.380\n",
            "   ìµœì € ì„±ê³µë¥ : 0.000\n",
            "   ì„±ê³µë¥  íŽ¸ì°¨: 0.154\n",
            "\n",
            "ðŸ”¸ ë‚œìž ì¶œì²˜:\n",
            "   ì¹´í…Œê³ ë¦¬ ìˆ˜: 3ê°œ\n",
            "   ìµœê³  ì„±ê³µë¥ : 0.315\n",
            "   ìµœì € ì„±ê³µë¥ : 0.129\n",
            "   ì„±ê³µë¥  íŽ¸ì°¨: 0.096\n",
            "\n",
            "ðŸ”¸ ì •ìž ì¶œì²˜:\n",
            "   ì¹´í…Œê³ ë¦¬ ìˆ˜: 4ê°œ\n",
            "   ìµœê³  ì„±ê³µë¥ : 0.260\n",
            "   ìµœì € ì„±ê³µë¥ : 0.000\n",
            "   ì„±ê³µë¥  íŽ¸ì°¨: 0.123\n",
            "\n",
            "==================================================\n",
            "ðŸ“ˆ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ ë¶„ì„\n",
            "==================================================\n",
            "ðŸ“Š ì£¼ìš” ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í†µê³„:\n",
            "\n",
            "ðŸ”¸ ì´ ìƒì„± ë°°ì•„ ìˆ˜:\n",
            "   í‰ê· : 5.06\n",
            "   ì¤‘ìœ„ìˆ˜: 4.00\n",
            "   í‘œì¤€íŽ¸ì°¨: 4.66\n",
            "   ìµœëŒ“ê°’: 51\n",
            "   0ì¸ ë¹„ìœ¨: 21.33%\n",
            "   ì‹¤íŒ¨êµ° í‰ê· : 4.66\n",
            "   ì„±ê³µêµ° í‰ê· : 6.21\n",
            "   ì°¨ì´: 1.55\n",
            "\n",
            "ðŸ”¸ ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜:\n",
            "   í‰ê· : 4.17\n",
            "   ì¤‘ìœ„ìˆ˜: 0.00\n",
            "   í‘œì¤€íŽ¸ì°¨: 5.63\n",
            "   ìµœëŒ“ê°’: 51\n",
            "   0ì¸ ë¹„ìœ¨: 51.06%\n",
            "   ì‹¤íŒ¨êµ° í‰ê· : 3.94\n",
            "   ì„±ê³µêµ° í‰ê· : 4.83\n",
            "   ì°¨ì´: 0.90\n",
            "\n",
            "ðŸ”¸ ë¯¸ì„¸ì£¼ìž…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜:\n",
            "   í‰ê· : 2.84\n",
            "   ì¤‘ìœ„ìˆ˜: 0.00\n",
            "   í‘œì¤€íŽ¸ì°¨: 4.09\n",
            "   ìµœëŒ“ê°’: 43\n",
            "   0ì¸ ë¹„ìœ¨: 52.70%\n",
            "   ì‹¤íŒ¨êµ° í‰ê· : 2.62\n",
            "   ì„±ê³µêµ° í‰ê· : 3.46\n",
            "   ì°¨ì´: 0.84\n",
            "\n",
            "ðŸ”¸ ì´ì‹ëœ ë°°ì•„ ìˆ˜:\n",
            "   í‰ê· : 1.37\n",
            "   ì¤‘ìœ„ìˆ˜: 1.00\n",
            "   í‘œì¤€íŽ¸ì°¨: 0.77\n",
            "   ìµœëŒ“ê°’: 3\n",
            "   0ì¸ ë¹„ìœ¨: 14.61%\n",
            "   ì‹¤íŒ¨êµ° í‰ê· : 1.30\n",
            "   ì„±ê³µêµ° í‰ê· : 1.57\n",
            "   ì°¨ì´: 0.28\n",
            "\n",
            "ðŸ”¸ ë¯¸ì„¸ì£¼ìž… ë°°ì•„ ì´ì‹ ìˆ˜:\n",
            "   í‰ê· : 0.72\n",
            "   ì¤‘ìœ„ìˆ˜: 0.00\n",
            "   í‘œì¤€íŽ¸ì°¨: 0.89\n",
            "   ìµœëŒ“ê°’: 3\n",
            "   0ì¸ ë¹„ìœ¨: 55.50%\n",
            "   ì‹¤íŒ¨êµ° í‰ê· : 0.69\n",
            "   ì„±ê³µêµ° í‰ê· : 0.84\n",
            "   ì°¨ì´: 0.15\n",
            "\n",
            "ðŸ”¸ ì €ìž¥ëœ ë°°ì•„ ìˆ˜:\n",
            "   í‰ê· : 1.19\n",
            "   ì¤‘ìœ„ìˆ˜: 0.00\n",
            "   í‘œì¤€íŽ¸ì°¨: 2.50\n",
            "   ìµœëŒ“ê°’: 51\n",
            "   0ì¸ ë¹„ìœ¨: 66.73%\n",
            "   ì‹¤íŒ¨êµ° í‰ê· : 1.13\n",
            "   ì„±ê³µêµ° í‰ê· : 1.34\n",
            "   ì°¨ì´: 0.21\n",
            "\n",
            "ðŸ”¸ ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜:\n",
            "   í‰ê· : 8.56\n",
            "   ì¤‘ìœ„ìˆ˜: 8.00\n",
            "   í‘œì¤€íŽ¸ì°¨: 7.32\n",
            "   ìµœëŒ“ê°’: 51\n",
            "   0ì¸ ë¹„ìœ¨: 21.53%\n",
            "   ì‹¤íŒ¨êµ° í‰ê· : 8.20\n",
            "   ì„±ê³µêµ° í‰ê· : 9.58\n",
            "   ì°¨ì´: 1.38\n",
            "\n",
            "ðŸ”¸ í˜¼í•©ëœ ë‚œìž ìˆ˜:\n",
            "   í‰ê· : 7.70\n",
            "   ì¤‘ìœ„ìˆ˜: 7.00\n",
            "   í‘œì¤€íŽ¸ì°¨: 6.45\n",
            "   ìµœëŒ“ê°’: 51\n",
            "   0ì¸ ë¹„ìœ¨: 18.66%\n",
            "   ì‹¤íŒ¨êµ° í‰ê· : 7.25\n",
            "   ì„±ê³µêµ° í‰ê· : 8.96\n",
            "   ì°¨ì´: 1.70\n",
            "\n",
            "ðŸ”¸ íŒŒíŠ¸ë„ˆ ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜:\n",
            "   í‰ê· : 7.11\n",
            "   ì¤‘ìœ„ìˆ˜: 6.00\n",
            "   í‘œì¤€íŽ¸ì°¨: 6.53\n",
            "   ìµœëŒ“ê°’: 51\n",
            "   0ì¸ ë¹„ìœ¨: 25.16%\n",
            "   ì‹¤íŒ¨êµ° í‰ê· : 6.70\n",
            "   ì„±ê³µêµ° í‰ê· : 8.26\n",
            "   ì°¨ì´: 1.56\n",
            "\n",
            "==================================================\n",
            "ðŸ”— ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„\n",
            "==================================================\n",
            "ðŸ“Š ë†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.7) ë³€ìˆ˜ ìŒ: 7ê°œ\n",
            "   ì´ ìƒì„± ë°°ì•„ ìˆ˜ â†” ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜: 0.779\n",
            "   ì´ ìƒì„± ë°°ì•„ ìˆ˜ â†” í˜¼í•©ëœ ë‚œìž ìˆ˜: 0.892\n",
            "   ì´ ìƒì„± ë°°ì•„ ìˆ˜ â†” íŒŒíŠ¸ë„ˆ ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜: 0.811\n",
            "   ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜ â†” ë¯¸ì„¸ì£¼ìž…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜: 0.940\n",
            "   ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜ â†” í˜¼í•©ëœ ë‚œìž ìˆ˜: 0.872\n",
            "   ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜ â†” íŒŒíŠ¸ë„ˆ ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜: 0.810\n",
            "   í˜¼í•©ëœ ë‚œìž ìˆ˜ â†” íŒŒíŠ¸ë„ˆ ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜: 0.913\n",
            "\n",
            "==================================================\n",
            "ðŸ“Š ì´ìƒì¹˜ íƒì§€ (IQR ë°©ë²•)\n",
            "==================================================\n",
            "ðŸ“Š ì´ìƒì¹˜ í˜„í™©:\n",
            "   ì €ìž¥ëœ ë°°ì•„ ìˆ˜: 14218ê°œ (5.7%)\n",
            "\n",
            "==================================================\n",
            "ðŸŽ¯ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ìƒí˜¸ì •ë³´ëŸ‰)\n",
            "==================================================\n",
            "ðŸ“Š ìƒí˜¸ì •ë³´ëŸ‰ ê¸°ë°˜ íŠ¹ì„± ì¤‘ìš”ë„ (Top 10):\n",
            "   ì´ì‹ëœ ë°°ì•„ ìˆ˜: 0.0607\n",
            "   ì´ ìƒì„± ë°°ì•„ ìˆ˜: 0.0208\n",
            "   í˜¼í•©ëœ ë‚œìž ìˆ˜: 0.0177\n",
            "   íŒŒíŠ¸ë„ˆ ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜: 0.0150\n",
            "   ë¯¸ì„¸ì£¼ìž… ë°°ì•„ ì´ì‹ ìˆ˜: 0.0142\n",
            "   ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜: 0.0138\n",
            "   ì €ìž¥ëœ ë°°ì•„ ìˆ˜: 0.0120\n",
            "   ë¯¸ì„¸ì£¼ìž…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜: 0.0101\n",
            "   ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜: 0.0100\n",
            "\n",
            "======================================================================\n",
            "âœ… 3ë‹¨ê³„ ì™„ë£Œ: ìƒì„¸ EDA ë° ë¶„ì„\n",
            "ðŸ”„ ë‹¤ìŒ ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\n",
            "======================================================================\n",
            "\n",
            "ðŸ“‹ EDA ìš”ì•½:\n",
            "   total_samples: 256351\n",
            "   total_features: 67\n",
            "   target_success_rate: 0.2583489044318142\n",
            "   imbalance_ratio: 2.870734432566286\n",
            "   high_missing_features: 6\n",
            "   medium_missing_features: 2\n",
            "   low_missing_features: 23\n",
            "   no_missing_features: 36\n",
            "   high_correlation_pairs: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§"
      ],
      "metadata": {
        "id": "9OajV1U_ztJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# ìž„ì‹  ì„±ê³µ ì—¬ë¶€ ì˜ˆì¸¡ ëª¨ë¸ - ì„±ëŠ¥ í–¥ìƒ ë²„ì „\n",
        "# 4ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\n",
        "# ====================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ”§ 4ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹œìž‘\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. ê³ ê²°ì¸¡ë¥  ë³€ìˆ˜ ì œê±° (80% ì´ìƒ)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ—‘ï¸ ê³ ê²°ì¸¡ë¥  ë³€ìˆ˜ ì œê±°\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# EDAì—ì„œ í™•ì¸ëœ ê³ ê²°ì¸¡ë¥  ë³€ìˆ˜ë“¤\n",
        "high_missing_cols = [\n",
        "    'ë‚œìž í•´ë™ ê²½ê³¼ì¼', 'PGS ì‹œìˆ  ì—¬ë¶€', 'PGD ì‹œìˆ  ì—¬ë¶€',\n",
        "    'ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€', 'ìž„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ìž„ì‹  ê²½ê³¼ ì—°ìˆ˜', 'ë°°ì•„ í•´ë™ ê²½ê³¼ì¼'\n",
        "]\n",
        "\n",
        "print(f\"ì œê±°í•  ê³ ê²°ì¸¡ë¥  ë³€ìˆ˜: {len(high_missing_cols)}ê°œ\")\n",
        "for col in high_missing_cols:\n",
        "    if col in X.columns:\n",
        "        missing_pct = (X[col].isnull().sum() / len(X)) * 100\n",
        "        print(f\"   - {col}: {missing_pct:.1f}% ê²°ì¸¡\")\n",
        "\n",
        "# ë³€ìˆ˜ ì œê±°\n",
        "X_processed = X.drop(columns=[col for col in high_missing_cols if col in X.columns])\n",
        "test_processed = test.drop(columns=[col for col in high_missing_cols if col in test.columns])\n",
        "\n",
        "print(f\"\\nâœ… ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "print(f\"   ì´ì „: {X.shape[1]}ê°œ â†’ í˜„ìž¬: {X_processed.shape[1]}ê°œ ë³€ìˆ˜\")\n",
        "\n",
        "# 2. ë°ì´í„° íƒ€ìž…ë³„ ë¶„ë¥˜\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ“Š ë°ì´í„° íƒ€ìž…ë³„ ë³€ìˆ˜ ë¶„ë¥˜\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ë²”ì£¼í˜• ë³€ìˆ˜ (object íƒ€ìž… + íŠ¹ì • ìˆ˜ì¹˜í˜•)\n",
        "categorical_cols = []\n",
        "numerical_cols = []\n",
        "\n",
        "for col in X_processed.columns:\n",
        "    if X_processed[col].dtype == 'object':\n",
        "        categorical_cols.append(col)\n",
        "    elif X_processed[col].nunique() <= 10 and col.endswith(('ì—¬ë¶€', 'ì›ì¸', 'ìœ í˜•', 'íšŸìˆ˜', 'ì¶œì²˜', 'ë‚˜ì´')):\n",
        "        categorical_cols.append(col)\n",
        "    else:\n",
        "        numerical_cols.append(col)\n",
        "\n",
        "print(f\"ðŸ“ ë²”ì£¼í˜• ë³€ìˆ˜: {len(categorical_cols)}ê°œ\")\n",
        "print(f\"ðŸ“ˆ ìˆ˜ì¹˜í˜• ë³€ìˆ˜: {len(numerical_cols)}ê°œ\")\n",
        "\n",
        "# 3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "X_processed = X_processed.copy()\n",
        "test_processed = test_processed.copy()\n",
        "\n",
        "# ë²”ì£¼í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´)\n",
        "for col in categorical_cols:\n",
        "    if col in X_processed.columns:\n",
        "        missing_count = X_processed[col].isnull().sum()\n",
        "        if missing_count > 0:\n",
        "            mode_value = X_processed[col].mode()[0] if len(X_processed[col].mode()) > 0 else 'Unknown'\n",
        "            X_processed[col] = X_processed[col].fillna(mode_value)\n",
        "            test_processed[col] = test_processed[col].fillna(mode_value)\n",
        "            print(f\"   ðŸ“ {col}: {missing_count}ê°œ â†’ '{mode_value}'ë¡œ ëŒ€ì²´\")\n",
        "\n",
        "# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì¤‘ìœ„ìˆ˜ë¡œ ëŒ€ì²´)\n",
        "for col in numerical_cols:\n",
        "    if col in X_processed.columns:\n",
        "        missing_count = X_processed[col].isnull().sum()\n",
        "        if missing_count > 0:\n",
        "            median_value = X_processed[col].median()\n",
        "            X_processed[col] = X_processed[col].fillna(median_value)\n",
        "            test_processed[col] = test_processed[col].fillna(median_value)\n",
        "            print(f\"   ðŸ“ˆ {col}: {missing_count}ê°œ â†’ {median_value}ë¡œ ëŒ€ì²´\")\n",
        "\n",
        "print(f\"\\nâœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "\n",
        "# 4. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ - ë„ë©”ì¸ ì§€ì‹ í™œìš©\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ§¬ ë„ë©”ì¸ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "def add_engineered_features(df):\n",
        "    \"\"\"ì˜ë£Œ ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ íŠ¹ì„± ìƒì„±\"\"\"\n",
        "    df_new = df.copy()\n",
        "\n",
        "    # 1. íš¨ìœ¨ì„± ì§€í‘œë“¤\n",
        "    if 'ì´ ìƒì„± ë°°ì•„ ìˆ˜' in df.columns and 'ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜' in df.columns:\n",
        "        # ë°°ì•„ ìƒì„± íš¨ìœ¨ = ìƒì„±ëœ ë°°ì•„ ìˆ˜ / ìˆ˜ì§‘ëœ ë‚œìž ìˆ˜\n",
        "        df_new['ë°°ì•„_ìƒì„±_íš¨ìœ¨'] = df['ì´ ìƒì„± ë°°ì•„ ìˆ˜'] / (df['ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜'] + 1)\n",
        "\n",
        "    if 'ì´ì‹ëœ ë°°ì•„ ìˆ˜' in df.columns and 'ì´ ìƒì„± ë°°ì•„ ìˆ˜' in df.columns:\n",
        "        # ë°°ì•„ ì´ì‹ ë¹„ìœ¨ = ì´ì‹ëœ ë°°ì•„ ìˆ˜ / ìƒì„±ëœ ë°°ì•„ ìˆ˜\n",
        "        df_new['ë°°ì•„_ì´ì‹_ë¹„ìœ¨'] = df['ì´ì‹ëœ ë°°ì•„ ìˆ˜'] / (df['ì´ ìƒì„± ë°°ì•„ ìˆ˜'] + 1)\n",
        "\n",
        "    if 'ì €ìž¥ëœ ë°°ì•„ ìˆ˜' in df.columns and 'ì´ ìƒì„± ë°°ì•„ ìˆ˜' in df.columns:\n",
        "        # ë°°ì•„ ë³´ì¡´ ë¹„ìœ¨ = ì €ìž¥ëœ ë°°ì•„ ìˆ˜ / ìƒì„±ëœ ë°°ì•„ ìˆ˜\n",
        "        df_new['ë°°ì•„_ë³´ì¡´_ë¹„ìœ¨'] = df['ì €ìž¥ëœ ë°°ì•„ ìˆ˜'] / (df['ì´ ìƒì„± ë°°ì•„ ìˆ˜'] + 1)\n",
        "\n",
        "    # 2. ë¯¸ì„¸ì£¼ìž… ê´€ë ¨ ì§€í‘œ\n",
        "    if 'ë¯¸ì„¸ì£¼ìž…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜' in df.columns and 'ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜' in df.columns:\n",
        "        # ë¯¸ì„¸ì£¼ìž… ì„±ê³µë¥  = ë¯¸ì„¸ì£¼ìž… ìƒì„± ë°°ì•„ / ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž\n",
        "        df_new['ë¯¸ì„¸ì£¼ìž…_ì„±ê³µë¥ '] = df['ë¯¸ì„¸ì£¼ìž…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜'] / (df['ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜'] + 1)\n",
        "\n",
        "    # 3. ì¢…í•© ì¹˜ë£Œ ê°•ë„ ì§€í‘œ\n",
        "    treatment_intensity_cols = [\n",
        "        'ì´ ì‹œìˆ  íšŸìˆ˜', 'IVF ì‹œìˆ  íšŸìˆ˜', 'DI ì‹œìˆ  íšŸìˆ˜'\n",
        "    ]\n",
        "    available_cols = [col for col in treatment_intensity_cols if col in df.columns]\n",
        "    if available_cols:\n",
        "        # ì¹˜ë£Œ ê°•ë„ ì ìˆ˜ (ì¹´í…Œê³ ë¦¬ë¥¼ ìˆ˜ì¹˜ë¡œ ë³€í™˜ í›„ í•©ì‚°)\n",
        "        treatment_scores = []\n",
        "        for col in available_cols:\n",
        "            if df[col].dtype == 'object':\n",
        "                # '0íšŒ', '1íšŒ' ë“±ì„ ìˆ«ìžë¡œ ë³€í™˜\n",
        "                col_numeric = df[col].str.extract(r'(\\d+)').astype(float).fillna(0)\n",
        "                treatment_scores.append(col_numeric.iloc[:, 0])\n",
        "            else:\n",
        "                treatment_scores.append(df[col])\n",
        "\n",
        "        if treatment_scores:\n",
        "            df_new['ì¹˜ë£Œ_ê°•ë„_ì ìˆ˜'] = sum(treatment_scores)\n",
        "\n",
        "    # 4. ë‚˜ì´ ê·¸ë£¹ ì¸ì½”ë”© (ìˆœì„œí˜•)\n",
        "    if 'ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´' in df.columns:\n",
        "        age_mapping = {\n",
        "            'ë§Œ18-34ì„¸': 1, 'ë§Œ35-37ì„¸': 2, 'ë§Œ38-39ì„¸': 3,\n",
        "            'ë§Œ40-42ì„¸': 4, 'ë§Œ43-44ì„¸': 5, 'ë§Œ45-50ì„¸': 6,\n",
        "            'ì•Œ ìˆ˜ ì—†ìŒ': 0\n",
        "        }\n",
        "        df_new['ë‚˜ì´_ê·¸ë£¹_ì ìˆ˜'] = df['ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´'].map(age_mapping).fillna(0)\n",
        "\n",
        "    # 5. ë¶ˆìž„ ì›ì¸ ì¢…í•© ì ìˆ˜\n",
        "    infertility_cols = [col for col in df.columns if 'ë¶ˆìž„ ì›ì¸' in col and df[col].dtype in ['int64', 'float64']]\n",
        "    if infertility_cols:\n",
        "        df_new['ë¶ˆìž„_ì›ì¸_ì´ê°œìˆ˜'] = df[infertility_cols].sum(axis=1)\n",
        "\n",
        "    # 6. ë°°ì•„/ë‚œìž í’ˆì§ˆ ì§€í‘œ\n",
        "    if 'ì´ ìƒì„± ë°°ì•„ ìˆ˜' in df.columns and 'í˜¼í•©ëœ ë‚œìž ìˆ˜' in df.columns:\n",
        "        # ì „ì²´ì ì¸ ìƒì‹ ì„¸í¬ í™œìš©ë„\n",
        "        df_new['ìƒì‹ì„¸í¬_í™œìš©ë„'] = (df['ì´ ìƒì„± ë°°ì•„ ìˆ˜'] + df['í˜¼í•©ëœ ë‚œìž ìˆ˜']) / 2\n",
        "\n",
        "    return df_new\n",
        "\n",
        "# íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš©\n",
        "print(\"ðŸ”¬ ë„ë©”ì¸ íŠ¹ì„± ìƒì„± ì¤‘...\")\n",
        "X_engineered = add_engineered_features(X_processed)\n",
        "test_engineered = add_engineered_features(test_processed)\n",
        "\n",
        "new_features = set(X_engineered.columns) - set(X_processed.columns)\n",
        "print(f\"âœ… ìƒì„±ëœ ìƒˆë¡œìš´ íŠ¹ì„±: {len(new_features)}ê°œ\")\n",
        "for feature in new_features:\n",
        "    print(f\"   + {feature}\")\n",
        "\n",
        "# 5. ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ·ï¸ ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "X_encoded = X_engineered.copy()\n",
        "test_encoded = test_engineered.copy()\n",
        "\n",
        "# ìˆœì„œê°€ ìžˆëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ë“¤ (ì´ë¯¸ ì²˜ë¦¬ë¨: ë‚˜ì´_ê·¸ë£¹_ì ìˆ˜)\n",
        "ordinal_features = ['ë‚˜ì´_ê·¸ë£¹_ì ìˆ˜'] if 'ë‚˜ì´_ê·¸ë£¹_ì ìˆ˜' in X_encoded.columns else []\n",
        "\n",
        "# ë‚˜ë¨¸ì§€ ë²”ì£¼í˜• ë³€ìˆ˜ë“¤\n",
        "remaining_categorical = [col for col in categorical_cols if col in X_encoded.columns and col not in ordinal_features]\n",
        "\n",
        "# Target Encoding ì ìš© (ì„±ëŠ¥ì´ ì¢‹ì€ ì¸ì½”ë”© ë°©ë²•)\n",
        "from sklearn.preprocessing import TargetEncoder\n",
        "\n",
        "if remaining_categorical:\n",
        "    print(f\"ðŸŽ¯ Target Encoding ì ìš© ì¤‘... ({len(remaining_categorical)}ê°œ ë³€ìˆ˜)\")\n",
        "\n",
        "    target_encoder = TargetEncoder(smooth=1.0, random_state=42)\n",
        "\n",
        "    # í•™ìŠµ ë°ì´í„°ë¡œ í”¼íŒ…\n",
        "    X_encoded[remaining_categorical] = target_encoder.fit_transform(\n",
        "        X_encoded[remaining_categorical], y\n",
        "    )\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜\n",
        "    test_encoded[remaining_categorical] = target_encoder.transform(\n",
        "        test_encoded[remaining_categorical]\n",
        "    )\n",
        "\n",
        "    print(\"âœ… Target Encoding ì™„ë£Œ\")\n",
        "\n",
        "# 6. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ“ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ëª¨ë“  ë³€ìˆ˜ê°€ ì´ì œ ìˆ˜ì¹˜í˜•ì´ë¯€ë¡œ ìŠ¤ì¼€ì¼ë§ ì ìš©\n",
        "scaler = RobustScaler()  # ì´ìƒì¹˜ì— ê°•ê±´í•œ ìŠ¤ì¼€ì¼ëŸ¬\n",
        "\n",
        "X_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X_encoded),\n",
        "    columns=X_encoded.columns,\n",
        "    index=X_encoded.index\n",
        ")\n",
        "\n",
        "test_scaled = pd.DataFrame(\n",
        "    scaler.transform(test_encoded),\n",
        "    columns=test_encoded.columns,\n",
        "    index=test_encoded.index\n",
        ")\n",
        "\n",
        "print(f\"âœ… RobustScaler ì ìš© ì™„ë£Œ\")\n",
        "print(f\"   íŠ¹ì„± ìˆ˜: {X_scaled.shape[1]}ê°œ\")\n",
        "\n",
        "# 7. ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° - ìƒê´€ê´€ê³„ ë†’ì€ ë³€ìˆ˜ ì œê±°\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ”— ë‹¤ì¤‘ê³µì„ ì„± í•´ê²°\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "correlation_matrix = X_scaled.corr().abs()\n",
        "high_corr_pairs = []\n",
        "\n",
        "# ìƒê´€ê´€ê³„ê°€ 0.9 ì´ìƒì¸ ë³€ìˆ˜ ìŒ ì°¾ê¸°\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        if correlation_matrix.iloc[i, j] > 0.9:\n",
        "            high_corr_pairs.append({\n",
        "                'var1': correlation_matrix.columns[i],\n",
        "                'var2': correlation_matrix.columns[j],\n",
        "                'correlation': correlation_matrix.iloc[i, j]\n",
        "            })\n",
        "\n",
        "print(f\"ë°œê²¬ëœ ê³ ìƒê´€ ë³€ìˆ˜ ìŒ: {len(high_corr_pairs)}ê°œ\")\n",
        "\n",
        "# ìƒê´€ê´€ê³„ê°€ ë†’ì€ ë³€ìˆ˜ ì¤‘ í•˜ë‚˜ì”© ì œê±°\n",
        "cols_to_remove = []\n",
        "for pair in high_corr_pairs:\n",
        "    var1, var2 = pair['var1'], pair['var2']\n",
        "\n",
        "    # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¹„êµí•˜ì—¬ ë” ë‚®ì€ ê²ƒ ì œê±°\n",
        "    corr_y_var1 = abs(X_scaled[var1].corr(y))\n",
        "    corr_y_var2 = abs(X_scaled[var2].corr(y))\n",
        "\n",
        "    if corr_y_var1 < corr_y_var2 and var1 not in cols_to_remove:\n",
        "        cols_to_remove.append(var1)\n",
        "        print(f\"   ì œê±°: {var1} (ìƒê´€: {pair['correlation']:.3f}, íƒ€ê²Ÿ ìƒê´€: {corr_y_var1:.3f})\")\n",
        "    elif var2 not in cols_to_remove:\n",
        "        cols_to_remove.append(var2)\n",
        "        print(f\"   ì œê±°: {var2} (ìƒê´€: {pair['correlation']:.3f}, íƒ€ê²Ÿ ìƒê´€: {corr_y_var2:.3f})\")\n",
        "\n",
        "# ë³€ìˆ˜ ì œê±° ì ìš©\n",
        "if cols_to_remove:\n",
        "    X_final = X_scaled.drop(columns=cols_to_remove)\n",
        "    test_final = test_scaled.drop(columns=cols_to_remove)\n",
        "    print(f\"\\nâœ… ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° ì™„ë£Œ: {len(cols_to_remove)}ê°œ ë³€ìˆ˜ ì œê±°\")\n",
        "else:\n",
        "    X_final = X_scaled\n",
        "    test_final = test_scaled\n",
        "    print(f\"\\nâœ… ì œê±°í•  ê³ ìƒê´€ ë³€ìˆ˜ ì—†ìŒ\")\n",
        "\n",
        "# 8. ìµœì¢… ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ“‹ ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "preprocessing_summary = {\n",
        "    'ì›ë³¸_íŠ¹ì„±ìˆ˜': X.shape[1],\n",
        "    'ê³ ê²°ì¸¡_ì œê±°í›„': X_processed.shape[1],\n",
        "    'íŠ¹ì„±ì—”ì§€ë‹ˆì–´ë§í›„': X_engineered.shape[1],\n",
        "    'ìµœì¢…_íŠ¹ì„±ìˆ˜': X_final.shape[1],\n",
        "    'ì œê±°ëœ_íŠ¹ì„±ìˆ˜': X.shape[1] - X_final.shape[1],\n",
        "    'ì¶”ê°€ëœ_íŠ¹ì„±ìˆ˜': X_final.shape[1] - X_processed.shape[1]\n",
        "}\n",
        "\n",
        "print(\"ðŸ“Š íŠ¹ì„± ë³€í™”:\")\n",
        "for key, value in preprocessing_summary.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ ìµœì¢… ë°ì´í„° í˜•íƒœ:\")\n",
        "print(f\"   Train: {X_final.shape}\")\n",
        "print(f\"   Test: {test_final.shape}\")\n",
        "print(f\"   Target: {y.shape}\")\n",
        "\n",
        "# ë°ì´í„° í’ˆì§ˆ ì²´í¬\n",
        "print(f\"\\nðŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:\")\n",
        "print(f\"   Train ê²°ì¸¡ì¹˜: {X_final.isnull().sum().sum()}ê°œ\")\n",
        "print(f\"   Test ê²°ì¸¡ì¹˜: {test_final.isnull().sum().sum()}ê°œ\")\n",
        "print(f\"   ë¬´í•œê°’ ì—¬ë¶€: {np.isinf(X_final).sum().sum()}ê°œ\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… 4ë‹¨ê³„ ì™„ë£Œ: ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\")\n",
        "print(\"ðŸ”„ ë‹¤ìŒ ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ ìµœì í™”\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë³€ìˆ˜ì— ì €ìž¥ (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš©)\n",
        "print(\"\\nðŸ’¾ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ìž¥ ì™„ë£Œ\")\n",
        "print(\"   ë³€ìˆ˜ëª…: X_final, test_final, y\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6ULzDzCzuGy",
        "outputId": "c6e6ec53-26a1-480a-c883-24a127f83432"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ðŸ”§ 4ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹œìž‘\n",
            "======================================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ—‘ï¸ ê³ ê²°ì¸¡ë¥  ë³€ìˆ˜ ì œê±°\n",
            "==================================================\n",
            "ì œê±°í•  ê³ ê²°ì¸¡ë¥  ë³€ìˆ˜: 6ê°œ\n",
            "   - ë‚œìž í•´ë™ ê²½ê³¼ì¼: 99.4% ê²°ì¸¡\n",
            "   - PGS ì‹œìˆ  ì—¬ë¶€: 99.2% ê²°ì¸¡\n",
            "   - PGD ì‹œìˆ  ì—¬ë¶€: 99.1% ê²°ì¸¡\n",
            "   - ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€: 98.9% ê²°ì¸¡\n",
            "   - ìž„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ìž„ì‹  ê²½ê³¼ ì—°ìˆ˜: 96.3% ê²°ì¸¡\n",
            "   - ë°°ì•„ í•´ë™ ê²½ê³¼ì¼: 84.3% ê²°ì¸¡\n",
            "\n",
            "âœ… ì²˜ë¦¬ ì™„ë£Œ!\n",
            "   ì´ì „: 67ê°œ â†’ í˜„ìž¬: 61ê°œ ë³€ìˆ˜\n",
            "\n",
            "==================================================\n",
            "ðŸ“Š ë°ì´í„° íƒ€ìž…ë³„ ë³€ìˆ˜ ë¶„ë¥˜\n",
            "==================================================\n",
            "ðŸ“ ë²”ì£¼í˜• ë³€ìˆ˜: 34ê°œ\n",
            "ðŸ“ˆ ìˆ˜ì¹˜í˜• ë³€ìˆ˜: 27ê°œ\n",
            "\n",
            "==================================================\n",
            "ðŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
            "==================================================\n",
            "   ðŸ“ íŠ¹ì • ì‹œìˆ  ìœ í˜•: 2ê°œ â†’ 'ICSI'ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ ë‹¨ì¼ ë°°ì•„ ì´ì‹ ì—¬ë¶€: 6291ê°œ â†’ '0.0'ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€: 6291ê°œ â†’ '0.0'ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ ë°°ì•„ ìƒì„± ì£¼ìš” ì´ìœ : 6291ê°œ â†’ 'í˜„ìž¬ ì‹œìˆ ìš©'ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€: 6291ê°œ â†’ '0.0'ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€: 6291ê°œ â†’ '1.0'ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€: 6291ê°œ â†’ '0.0'ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ ëŒ€ë¦¬ëª¨ ì—¬ë¶€: 6291ê°œ â†’ '0.0'ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ ì´ ìƒì„± ë°°ì•„ ìˆ˜: 6291ê°œ â†’ 4.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜: 6291ê°œ â†’ 0.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ ë¯¸ì„¸ì£¼ìž…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜: 6291ê°œ â†’ 0.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ ì´ì‹ëœ ë°°ì•„ ìˆ˜: 6291ê°œ â†’ 1.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ ë¯¸ì„¸ì£¼ìž… ë°°ì•„ ì´ì‹ ìˆ˜: 6291ê°œ â†’ 0.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ ì €ìž¥ëœ ë°°ì•„ ìˆ˜: 6291ê°œ â†’ 0.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ ë¯¸ì„¸ì£¼ìž… í›„ ì €ìž¥ëœ ë°°ì•„ ìˆ˜: 6291ê°œ â†’ 0.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ í•´ë™ëœ ë°°ì•„ ìˆ˜: 6291ê°œ â†’ 0.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ í•´ë™ ë‚œìž ìˆ˜: 6291ê°œ â†’ 0.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜: 6291ê°œ â†’ 8.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ ì €ìž¥ëœ ì‹ ì„  ë‚œìž ìˆ˜: 6291ê°œ â†’ 0.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ í˜¼í•©ëœ ë‚œìž ìˆ˜: 6291ê°œ â†’ 7.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ íŒŒíŠ¸ë„ˆ ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜: 6291ê°œ â†’ 6.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ ê¸°ì¦ìž ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜: 6291ê°œ â†’ 0.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ ë‚œìž ì±„ì·¨ ê²½ê³¼ì¼: 57488ê°œ â†’ 0.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ ë‚œìž í˜¼í•© ê²½ê³¼ì¼: 53735ê°œ â†’ 0.0ë¡œ ëŒ€ì²´\n",
            "   ðŸ“ˆ ë°°ì•„ ì´ì‹ ê²½ê³¼ì¼: 43566ê°œ â†’ 3.0ë¡œ ëŒ€ì²´\n",
            "\n",
            "âœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\n",
            "\n",
            "==================================================\n",
            "ðŸ§¬ ë„ë©”ì¸ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\n",
            "==================================================\n",
            "ðŸ”¬ ë„ë©”ì¸ íŠ¹ì„± ìƒì„± ì¤‘...\n",
            "âœ… ìƒì„±ëœ ìƒˆë¡œìš´ íŠ¹ì„±: 8ê°œ\n",
            "   + ë°°ì•„_ìƒì„±_íš¨ìœ¨\n",
            "   + ë¶ˆìž„_ì›ì¸_ì´ê°œìˆ˜\n",
            "   + ìƒì‹ì„¸í¬_í™œìš©ë„\n",
            "   + ë‚˜ì´_ê·¸ë£¹_ì ìˆ˜\n",
            "   + ë¯¸ì„¸ì£¼ìž…_ì„±ê³µë¥ \n",
            "   + ë°°ì•„_ë³´ì¡´_ë¹„ìœ¨\n",
            "   + ì¹˜ë£Œ_ê°•ë„_ì ìˆ˜\n",
            "   + ë°°ì•„_ì´ì‹_ë¹„ìœ¨\n",
            "\n",
            "==================================================\n",
            "ðŸ·ï¸ ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©\n",
            "==================================================\n",
            "ðŸŽ¯ Target Encoding ì ìš© ì¤‘... (34ê°œ ë³€ìˆ˜)\n",
            "âœ… Target Encoding ì™„ë£Œ\n",
            "\n",
            "==================================================\n",
            "ðŸ“ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§\n",
            "==================================================\n",
            "âœ… RobustScaler ì ìš© ì™„ë£Œ\n",
            "   íŠ¹ì„± ìˆ˜: 69ê°œ\n",
            "\n",
            "==================================================\n",
            "ðŸ”— ë‹¤ì¤‘ê³µì„ ì„± í•´ê²°\n",
            "==================================================\n",
            "ë°œê²¬ëœ ê³ ìƒê´€ ë³€ìˆ˜ ìŒ: 11ê°œ\n",
            "   ì œê±°: ë‚˜ì´_ê·¸ë£¹_ì ìˆ˜ (ìƒê´€: 0.951, íƒ€ê²Ÿ ìƒê´€: 0.149)\n",
            "   ì œê±°: ë°°ëž€ ìžê·¹ ì—¬ë¶€ (ìƒê´€: 0.965, íƒ€ê²Ÿ ìƒê´€: 0.033)\n",
            "   ì œê±°: ë‚¨ì„± ì£¼ ë¶ˆìž„ ì›ì¸ (ìƒê´€: 0.914, íƒ€ê²Ÿ ìƒê´€: 0.015)\n",
            "   ì œê±°: IVF ì‹œìˆ  íšŸìˆ˜ (ìƒê´€: 0.930, íƒ€ê²Ÿ ìƒê´€: 0.059)\n",
            "   ì œê±°: ì¹˜ë£Œ_ê°•ë„_ì ìˆ˜ (ìƒê´€: 0.917, íƒ€ê²Ÿ ìƒê´€: 0.059)\n",
            "   ì œê±°: ì´ ì¶œì‚° íšŸìˆ˜ (ìƒê´€: 0.951, íƒ€ê²Ÿ ìƒê´€: 0.006)\n",
            "   ì œê±°: ìƒì‹ì„¸í¬_í™œìš©ë„ (ìƒê´€: 0.963, íƒ€ê²Ÿ ìƒê´€: 0.132)\n",
            "   ì œê±°: ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜ (ìƒê´€: 0.941, íƒ€ê²Ÿ ìƒê´€: 0.074)\n",
            "   ì œê±°: íŒŒíŠ¸ë„ˆ ì •ìžì™€ í˜¼í•©ëœ ë‚œìž ìˆ˜ (ìƒê´€: 0.913, íƒ€ê²Ÿ ìƒê´€: 0.105)\n",
            "   ì œê±°: í˜¼í•©ëœ ë‚œìž ìˆ˜ (ìƒê´€: 0.981, íƒ€ê²Ÿ ìƒê´€: 0.116)\n",
            "   ì œê±°: ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€ (ìƒê´€: 0.997, íƒ€ê²Ÿ ìƒê´€: 0.028)\n",
            "\n",
            "âœ… ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° ì™„ë£Œ: 11ê°œ ë³€ìˆ˜ ì œê±°\n",
            "\n",
            "==================================================\n",
            "ðŸ“‹ ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½\n",
            "==================================================\n",
            "ðŸ“Š íŠ¹ì„± ë³€í™”:\n",
            "   ì›ë³¸_íŠ¹ì„±ìˆ˜: 67\n",
            "   ê³ ê²°ì¸¡_ì œê±°í›„: 61\n",
            "   íŠ¹ì„±ì—”ì§€ë‹ˆì–´ë§í›„: 69\n",
            "   ìµœì¢…_íŠ¹ì„±ìˆ˜: 58\n",
            "   ì œê±°ëœ_íŠ¹ì„±ìˆ˜: 9\n",
            "   ì¶”ê°€ëœ_íŠ¹ì„±ìˆ˜: -3\n",
            "\n",
            "ðŸ“ˆ ìµœì¢… ë°ì´í„° í˜•íƒœ:\n",
            "   Train: (256351, 58)\n",
            "   Test: (90067, 58)\n",
            "   Target: (256351,)\n",
            "\n",
            "ðŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:\n",
            "   Train ê²°ì¸¡ì¹˜: 0ê°œ\n",
            "   Test ê²°ì¸¡ì¹˜: 0ê°œ\n",
            "   ë¬´í•œê°’ ì—¬ë¶€: 0ê°œ\n",
            "\n",
            "======================================================================\n",
            "âœ… 4ë‹¨ê³„ ì™„ë£Œ: ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\n",
            "ðŸ”„ ë‹¤ìŒ ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ ìµœì í™”\n",
            "======================================================================\n",
            "\n",
            "ðŸ’¾ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ìž¥ ì™„ë£Œ\n",
            "   ë³€ìˆ˜ëª…: X_final, test_final, y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5ë‹¨ê³„: êµì°¨ê²€ì¦ ì½”ë“œ"
      ],
      "metadata": {
        "id": "NJlq6r5t0Hiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# ìž„ì‹  ì„±ê³µ ì—¬ë¶€ ì˜ˆì¸¡ ëª¨ë¸ - ìˆ˜ì •ëœ ë²„ì „\n",
        "# 5ë‹¨ê³„: ì˜¬ë°”ë¥¸ êµì°¨ê²€ì¦ìœ¼ë¡œ ë°ì´í„° ë¦¬í‚¤ì§€ ë°©ì§€\n",
        "# ====================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ”§ 5ë‹¨ê³„ ìˆ˜ì •: ì˜¬ë°”ë¥¸ êµì°¨ê²€ì¦ íŒŒì´í”„ë¼ì¸\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ê¸°ì¡´ì˜ ìž˜ëª»ëœ ì „ì²˜ë¦¬ëœ ë°ì´í„° ëŒ€ì‹  ì›ë³¸ì—ì„œ ë‹¤ì‹œ ì‹œìž‘\n",
        "print(\"ðŸ“‚ ì›ë³¸ ë°ì´í„°ë¶€í„° ë‹¤ì‹œ ì‹œìž‘...\")\n",
        "\n",
        "# 1. ì›ë³¸ ë°ì´í„° ë¡œë“œ (4ë‹¨ê³„ ì´ˆê¸° ìƒíƒœ)\n",
        "train_raw = pd.read_csv('./train.csv').drop('ID', axis=1)\n",
        "test_raw = pd.read_csv('./test.csv').drop('ID', axis=1)\n",
        "\n",
        "X_raw = train_raw.drop('ìž„ì‹  ì„±ê³µ ì—¬ë¶€', axis=1)\n",
        "y_raw = train_raw['ìž„ì‹  ì„±ê³µ ì—¬ë¶€']\n",
        "\n",
        "print(f\"ì›ë³¸ ë°ì´í„°: {X_raw.shape}, íƒ€ê²Ÿ: {y_raw.shape}\")\n",
        "\n",
        "# 2. ê³ ê²°ì¸¡ë¥  ë³€ìˆ˜ ì œê±° (ì´ê±´ ì•ˆì „í•¨)\n",
        "high_missing_cols = [\n",
        "    'ë‚œìž í•´ë™ ê²½ê³¼ì¼', 'PGS ì‹œìˆ  ì—¬ë¶€', 'PGD ì‹œìˆ  ì—¬ë¶€',\n",
        "    'ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€', 'ìž„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ìž„ì‹  ê²½ê³¼ ì—°ìˆ˜', 'ë°°ì•„ í•´ë™ ê²½ê³¼ì¼'\n",
        "]\n",
        "\n",
        "X_clean = X_raw.drop(columns=[col for col in high_missing_cols if col in X_raw.columns])\n",
        "test_clean = test_raw.drop(columns=[col for col in high_missing_cols if col in test_raw.columns])\n",
        "\n",
        "print(f\"ê³ ê²°ì¸¡ ì œê±° í›„: {X_clean.shape}\")\n",
        "\n",
        "# 3. ì˜¬ë°”ë¥¸ êµì°¨ê²€ì¦ íŒŒì´í”„ë¼ì¸ í´ëž˜ìŠ¤\n",
        "class ProperCrossValidationPipeline:\n",
        "    def __init__(self):\n",
        "        self.categorical_cols = []\n",
        "        self.numerical_cols = []\n",
        "        self.preprocessors = {}\n",
        "\n",
        "    def identify_column_types(self, X):\n",
        "        \"\"\"ì»¬ëŸ¼ íƒ€ìž… ìžë™ ë¶„ë¥˜\"\"\"\n",
        "        categorical_cols = []\n",
        "        numerical_cols = []\n",
        "\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object':\n",
        "                categorical_cols.append(col)\n",
        "            elif X[col].nunique() <= 10 and col.endswith(('ì—¬ë¶€', 'ì›ì¸', 'ìœ í˜•', 'íšŸìˆ˜', 'ì¶œì²˜', 'ë‚˜ì´')):\n",
        "                categorical_cols.append(col)\n",
        "            else:\n",
        "                numerical_cols.append(col)\n",
        "\n",
        "        return categorical_cols, numerical_cols\n",
        "\n",
        "    def preprocess_data(self, X_train, X_val, y_train, fit_preprocessors=True):\n",
        "        \"\"\"ê° foldë§ˆë‹¤ ì˜¬ë°”ë¥´ê²Œ ì „ì²˜ë¦¬\"\"\"\n",
        "        X_train_processed = X_train.copy()\n",
        "        X_val_processed = X_val.copy()\n",
        "\n",
        "        if fit_preprocessors:\n",
        "            self.categorical_cols, self.numerical_cols = self.identify_column_types(X_train)\n",
        "\n",
        "        # 1. ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X_train_processed.columns:\n",
        "                if fit_preprocessors:\n",
        "                    mode_val = X_train_processed[col].mode()[0] if len(X_train_processed[col].mode()) > 0 else 'Unknown'\n",
        "                    self.preprocessors[f'{col}_mode'] = mode_val\n",
        "                else:\n",
        "                    mode_val = self.preprocessors[f'{col}_mode']\n",
        "\n",
        "                X_train_processed[col] = X_train_processed[col].fillna(mode_val)\n",
        "                X_val_processed[col] = X_val_processed[col].fillna(mode_val)\n",
        "\n",
        "        for col in self.numerical_cols:\n",
        "            if col in X_train_processed.columns:\n",
        "                if fit_preprocessors:\n",
        "                    median_val = X_train_processed[col].median()\n",
        "                    self.preprocessors[f'{col}_median'] = median_val\n",
        "                else:\n",
        "                    median_val = self.preprocessors[f'{col}_median']\n",
        "\n",
        "                X_train_processed[col] = X_train_processed[col].fillna(median_val)\n",
        "                X_val_processed[col] = X_val_processed[col].fillna(median_val)\n",
        "\n",
        "        # 2. ê°„ë‹¨í•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (ì•ˆì „í•œ ê²ƒë“¤ë§Œ)\n",
        "        self.add_safe_features(X_train_processed)\n",
        "        self.add_safe_features(X_val_processed)\n",
        "\n",
        "        # 3. Target Encoding (trainì—ì„œë§Œ í•™ìŠµ!)\n",
        "        remaining_categorical = [col for col in self.categorical_cols if col in X_train_processed.columns]\n",
        "\n",
        "        if remaining_categorical:\n",
        "            if fit_preprocessors:\n",
        "                # train ë°ì´í„°ë¡œë§Œ target encoding í•™ìŠµ\n",
        "                self.preprocessors['target_encoder'] = TargetEncoder(smooth=1.0, random_state=42)\n",
        "                X_train_processed[remaining_categorical] = self.preprocessors['target_encoder'].fit_transform(\n",
        "                    X_train_processed[remaining_categorical], y_train\n",
        "                )\n",
        "            else:\n",
        "                X_train_processed[remaining_categorical] = self.preprocessors['target_encoder'].transform(\n",
        "                    X_train_processed[remaining_categorical]\n",
        "                )\n",
        "\n",
        "            # validation ë°ì´í„°ëŠ” í•™ìŠµëœ ì¸ì½”ë”ë¡œë§Œ ë³€í™˜\n",
        "            X_val_processed[remaining_categorical] = self.preprocessors['target_encoder'].transform(\n",
        "                X_val_processed[remaining_categorical]\n",
        "            )\n",
        "\n",
        "        # 4. ìŠ¤ì¼€ì¼ë§ (trainì—ì„œë§Œ í•™ìŠµ!)\n",
        "        if fit_preprocessors:\n",
        "            self.preprocessors['scaler'] = RobustScaler()\n",
        "            X_train_scaled = pd.DataFrame(\n",
        "                self.preprocessors['scaler'].fit_transform(X_train_processed),\n",
        "                columns=X_train_processed.columns,\n",
        "                index=X_train_processed.index\n",
        "            )\n",
        "        else:\n",
        "            X_train_scaled = pd.DataFrame(\n",
        "                self.preprocessors['scaler'].transform(X_train_processed),\n",
        "                columns=X_train_processed.columns,\n",
        "                index=X_train_processed.index\n",
        "            )\n",
        "\n",
        "        X_val_scaled = pd.DataFrame(\n",
        "            self.preprocessors['scaler'].transform(X_val_processed),\n",
        "            columns=X_val_processed.columns,\n",
        "            index=X_val_processed.index\n",
        "        )\n",
        "\n",
        "        return X_train_scaled, X_val_scaled\n",
        "\n",
        "    def add_safe_features(self, df):\n",
        "        \"\"\"ì•ˆì „í•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (íƒ€ê²Ÿ ì •ë³´ ì‚¬ìš© ì•ˆí•¨)\"\"\"\n",
        "        # 1. íš¨ìœ¨ì„± ì§€í‘œë“¤\n",
        "        if 'ì´ ìƒì„± ë°°ì•„ ìˆ˜' in df.columns and 'ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜' in df.columns:\n",
        "            df['ë°°ì•„_ìƒì„±_íš¨ìœ¨'] = df['ì´ ìƒì„± ë°°ì•„ ìˆ˜'] / (df['ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜'] + 1)\n",
        "\n",
        "        if 'ì´ì‹ëœ ë°°ì•„ ìˆ˜' in df.columns and 'ì´ ìƒì„± ë°°ì•„ ìˆ˜' in df.columns:\n",
        "            df['ë°°ì•„_ì´ì‹_ë¹„ìœ¨'] = df['ì´ì‹ëœ ë°°ì•„ ìˆ˜'] / (df['ì´ ìƒì„± ë°°ì•„ ìˆ˜'] + 1)\n",
        "\n",
        "        # 2. ë‚˜ì´ ê·¸ë£¹ ì ìˆ˜ (ìˆœì„œí˜•)\n",
        "        if 'ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´' in df.columns:\n",
        "            age_mapping = {\n",
        "                'ë§Œ18-34ì„¸': 1, 'ë§Œ35-37ì„¸': 2, 'ë§Œ38-39ì„¸': 3,\n",
        "                'ë§Œ40-42ì„¸': 4, 'ë§Œ43-44ì„¸': 5, 'ë§Œ45-50ì„¸': 6,\n",
        "                'ì•Œ ìˆ˜ ì—†ìŒ': 0\n",
        "            }\n",
        "            df['ë‚˜ì´_ê·¸ë£¹_ì ìˆ˜'] = df['ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´'].map(age_mapping).fillna(0)\n",
        "\n",
        "# 4. ì˜¬ë°”ë¥¸ êµì°¨ê²€ì¦ ì‹¤í–‰\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ”„ ì˜¬ë°”ë¥¸ êµì°¨ê²€ì¦ ì‹œìž‘\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "cv_folds = 5\n",
        "skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”\n",
        "pipeline = ProperCrossValidationPipeline()\n",
        "\n",
        "# ê°„ë‹¨í•œ ëª¨ë¸ë“¤ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸\n",
        "models = {\n",
        "    'LightGBM': lgb.LGBMClassifier(\n",
        "        random_state=42,\n",
        "        n_estimators=100,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¤„ìž„\n",
        "        learning_rate=0.1,\n",
        "        num_leaves=31,\n",
        "        verbose=-1\n",
        "    ),\n",
        "    'RandomForest': RandomForestClassifier(\n",
        "        random_state=42,\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        min_samples_split=5\n",
        "    ),\n",
        "    'ExtraTrees': ExtraTreesClassifier(\n",
        "        random_state=42,\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        min_samples_split=5\n",
        "    )\n",
        "}\n",
        "\n",
        "model_scores = {}\n",
        "\n",
        "print(\"ðŸš€ ì˜¬ë°”ë¥¸ êµì°¨ê²€ì¦ ì‹œìž‘ (ë°ì´í„° ë¦¬í‚¤ì§€ ë°©ì§€)...\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nðŸ”„ {name} í•™ìŠµ ì¤‘...\")\n",
        "\n",
        "    cv_scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_clean, y_raw)):\n",
        "        # ë°ì´í„° ë¶„í• \n",
        "        X_train_fold = X_clean.iloc[train_idx]\n",
        "        X_val_fold = X_clean.iloc[val_idx]\n",
        "        y_train_fold = y_raw.iloc[train_idx]\n",
        "        y_val_fold = y_raw.iloc[val_idx]\n",
        "\n",
        "        # ê° foldë§ˆë‹¤ ìƒˆë¡œìš´ ì „ì²˜ë¦¬ (ì •ë³´ ëˆ„ì¶œ ë°©ì§€!)\n",
        "        X_train_processed, X_val_processed = pipeline.preprocess_data(\n",
        "            X_train_fold, X_val_fold, y_train_fold,\n",
        "            fit_preprocessors=(fold == 0)  # ì²« foldì—ì„œë§Œ íŒŒë¼ë¯¸í„° í•™ìŠµ\n",
        "        )\n",
        "\n",
        "        # SMOTEë„ trainì—ì„œë§Œ ì ìš©\n",
        "        smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed, y_train_fold)\n",
        "\n",
        "        # ëª¨ë¸ í•™ìŠµ\n",
        "        model_copy = type(model)(**model.get_params())\n",
        "        model_copy.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "        # ì˜ˆì¸¡ ë° í‰ê°€ (ì›ë³¸ validation setìœ¼ë¡œ!)\n",
        "        y_pred_proba = model_copy.predict_proba(X_val_processed)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_val_fold, y_pred_proba)\n",
        "\n",
        "        cv_scores.append(roc_auc)\n",
        "        print(f\"   Fold {fold+1}: {roc_auc:.4f}\")\n",
        "\n",
        "    avg_score = np.mean(cv_scores)\n",
        "    std_score = np.std(cv_scores)\n",
        "    model_scores[name] = {'mean': avg_score, 'std': std_score}\n",
        "\n",
        "    print(f\"   âœ… í‰ê·  ROC-AUC: {avg_score:.4f} (Â±{std_score:.4f})\")\n",
        "\n",
        "# 5. ê²°ê³¼ ë¹„êµ\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ† ìˆ˜ì •ëœ ê²°ê³¼\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "sorted_models = sorted(model_scores.items(), key=lambda x: x[1]['mean'], reverse=True)\n",
        "\n",
        "print(\"ðŸ“Š ROC-AUC ìˆœìœ„:\")\n",
        "for rank, (name, scores) in enumerate(sorted_models, 1):\n",
        "    print(f\"   {rank}. {name}: {scores['mean']:.4f} (Â±{scores['std']:.4f})\")\n",
        "\n",
        "best_model_name = sorted_models[0][0]\n",
        "best_score = sorted_models[0][1]['mean']\n",
        "\n",
        "print(f\"\\nðŸŽ¯ ì´ì œ ì •ìƒì ì¸ ì ìˆ˜ê°€ ë‚˜ì™”ëŠ”ì§€ í™•ì¸:\")\n",
        "print(f\"   ìµœê³  ì ìˆ˜: {best_score:.4f}\")\n",
        "\n",
        "if best_score > 0.5:\n",
        "    print(\"   âœ… ì •ìƒì ì¸ ì ìˆ˜ìž…ë‹ˆë‹¤! (0.5 ì´ìƒ)\")\n",
        "    print(\"   ðŸš€ ì´ì œ ë³¸ê²©ì ì¸ ëª¨ë¸ íŠœë‹ ì§„í–‰ ê°€ëŠ¥\")\n",
        "else:\n",
        "    print(\"   âš ï¸ ì—¬ì „ížˆ ë‚®ì€ ì ìˆ˜... ì¶”ê°€ ë””ë²„ê¹… í•„ìš”\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… ë°ì´í„° ë¦¬í‚¤ì§€ ìˆ˜ì • ì™„ë£Œ\")\n",
        "print(\"ðŸ”„ ì •ìƒ ì ìˆ˜ í™•ì¸ í›„ ë³¸ê²© íŠœë‹ ì§„í–‰\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkF8vGL9Cpv9",
        "outputId": "551e5164-b56d-453a-e38f-1b4e3e6e0ffb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ðŸ”§ 5ë‹¨ê³„ ìˆ˜ì •: ì˜¬ë°”ë¥¸ êµì°¨ê²€ì¦ íŒŒì´í”„ë¼ì¸\n",
            "======================================================================\n",
            "ðŸ“‚ ì›ë³¸ ë°ì´í„°ë¶€í„° ë‹¤ì‹œ ì‹œìž‘...\n",
            "ì›ë³¸ ë°ì´í„°: (256351, 67), íƒ€ê²Ÿ: (256351,)\n",
            "ê³ ê²°ì¸¡ ì œê±° í›„: (256351, 61)\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ ì˜¬ë°”ë¥¸ êµì°¨ê²€ì¦ ì‹œìž‘\n",
            "==================================================\n",
            "ðŸš€ ì˜¬ë°”ë¥¸ êµì°¨ê²€ì¦ ì‹œìž‘ (ë°ì´í„° ë¦¬í‚¤ì§€ ë°©ì§€)...\n",
            "\n",
            "ðŸ”„ LightGBM í•™ìŠµ ì¤‘...\n",
            "   Fold 1: 0.6983\n",
            "   Fold 2: 0.7404\n",
            "   Fold 3: 0.7375\n",
            "   Fold 4: 0.7360\n",
            "   Fold 5: 0.7375\n",
            "   âœ… í‰ê·  ROC-AUC: 0.7299 (Â±0.0159)\n",
            "\n",
            "ðŸ”„ RandomForest í•™ìŠµ ì¤‘...\n",
            "   Fold 1: 0.7273\n",
            "   Fold 2: 0.7325\n",
            "   Fold 3: 0.7305\n",
            "   Fold 4: 0.7288\n",
            "   Fold 5: 0.7299\n",
            "   âœ… í‰ê·  ROC-AUC: 0.7298 (Â±0.0017)\n",
            "\n",
            "ðŸ”„ ExtraTrees í•™ìŠµ ì¤‘...\n",
            "   Fold 1: 0.7237\n",
            "   Fold 2: 0.7290\n",
            "   Fold 3: 0.7269\n",
            "   Fold 4: 0.7268\n",
            "   Fold 5: 0.7239\n",
            "   âœ… í‰ê·  ROC-AUC: 0.7261 (Â±0.0020)\n",
            "\n",
            "==================================================\n",
            "ðŸ† ìˆ˜ì •ëœ ê²°ê³¼\n",
            "==================================================\n",
            "ðŸ“Š ROC-AUC ìˆœìœ„:\n",
            "   1. LightGBM: 0.7299 (Â±0.0159)\n",
            "   2. RandomForest: 0.7298 (Â±0.0017)\n",
            "   3. ExtraTrees: 0.7261 (Â±0.0020)\n",
            "\n",
            "ðŸŽ¯ ì´ì œ ì •ìƒì ì¸ ì ìˆ˜ê°€ ë‚˜ì™”ëŠ”ì§€ í™•ì¸:\n",
            "   ìµœê³  ì ìˆ˜: 0.7299\n",
            "   âœ… ì •ìƒì ì¸ ì ìˆ˜ìž…ë‹ˆë‹¤! (0.5 ì´ìƒ)\n",
            "   ðŸš€ ì´ì œ ë³¸ê²©ì ì¸ ëª¨ë¸ íŠœë‹ ì§„í–‰ ê°€ëŠ¥\n",
            "\n",
            "======================================================================\n",
            "âœ… ë°ì´í„° ë¦¬í‚¤ì§€ ìˆ˜ì • ì™„ë£Œ\n",
            "ðŸ”„ ì •ìƒ ì ìˆ˜ í™•ì¸ í›„ ë³¸ê²© íŠœë‹ ì§„í–‰\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.ê³ ì„±ëŠ¥ ëª¨ë¸"
      ],
      "metadata": {
        "id": "sNtU2V4pGFis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# ìµœì¢… ê³ ì„±ëŠ¥ ìž„ì‹  ì„±ê³µ ì˜ˆì¸¡ ëª¨ë¸ - ì™„ì „ì²´\n",
        "# ì˜¬ë°”ë¥¸ êµì°¨ê²€ì¦ + ê³ ê¸‰ íŠœë‹ + ì•™ìƒë¸”\n",
        "# ====================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ† ìµœì¢… ê³ ì„±ëŠ¥ ëª¨ë¸ - ì™„ì „ì²´ ì‹œìž‘\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. ë°ì´í„° ì¤€ë¹„ (ê²€ì¦ëœ ë°©ì‹)\n",
        "print(\"ðŸ“‚ ë°ì´í„° ì¤€ë¹„...\")\n",
        "train_raw = pd.read_csv('./train.csv').drop('ID', axis=1)\n",
        "test_raw = pd.read_csv('./test.csv').drop('ID', axis=1)\n",
        "\n",
        "X_raw = train_raw.drop('ìž„ì‹  ì„±ê³µ ì—¬ë¶€', axis=1)\n",
        "y_raw = train_raw['ìž„ì‹  ì„±ê³µ ì—¬ë¶€']\n",
        "\n",
        "# ê³ ê²°ì¸¡ë¥  ë³€ìˆ˜ ì œê±°\n",
        "high_missing_cols = [\n",
        "    'ë‚œìž í•´ë™ ê²½ê³¼ì¼', 'PGS ì‹œìˆ  ì—¬ë¶€', 'PGD ì‹œìˆ  ì—¬ë¶€',\n",
        "    'ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€', 'ìž„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ìž„ì‹  ê²½ê³¼ ì—°ìˆ˜', 'ë°°ì•„ í•´ë™ ê²½ê³¼ì¼'\n",
        "]\n",
        "\n",
        "X_clean = X_raw.drop(columns=[col for col in high_missing_cols if col in X_raw.columns])\n",
        "test_clean = test_raw.drop(columns=[col for col in high_missing_cols if col in test_raw.columns])\n",
        "\n",
        "print(f\"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_clean.shape}\")\n",
        "\n",
        "# 2. ê³ ê¸‰ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ëž˜ìŠ¤\n",
        "class AdvancedPreprocessingPipeline:\n",
        "    def __init__(self):\n",
        "        self.categorical_cols = []\n",
        "        self.numerical_cols = []\n",
        "        self.preprocessors = {}\n",
        "\n",
        "    def identify_column_types(self, X):\n",
        "        categorical_cols = []\n",
        "        numerical_cols = []\n",
        "\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object':\n",
        "                categorical_cols.append(col)\n",
        "            elif X[col].nunique() <= 10 and col.endswith(('ì—¬ë¶€', 'ì›ì¸', 'ìœ í˜•', 'íšŸìˆ˜', 'ì¶œì²˜', 'ë‚˜ì´')):\n",
        "                categorical_cols.append(col)\n",
        "            else:\n",
        "                numerical_cols.append(col)\n",
        "\n",
        "        return categorical_cols, numerical_cols\n",
        "\n",
        "    def preprocess_data(self, X_train, X_val, y_train, fit_preprocessors=True):\n",
        "        X_train_processed = X_train.copy()\n",
        "        X_val_processed = X_val.copy()\n",
        "\n",
        "        if fit_preprocessors:\n",
        "            self.categorical_cols, self.numerical_cols = self.identify_column_types(X_train)\n",
        "\n",
        "        # 1. ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X_train_processed.columns:\n",
        "                if fit_preprocessors:\n",
        "                    mode_val = X_train_processed[col].mode()[0] if len(X_train_processed[col].mode()) > 0 else 'Unknown'\n",
        "                    self.preprocessors[f'{col}_mode'] = mode_val\n",
        "                else:\n",
        "                    mode_val = self.preprocessors[f'{col}_mode']\n",
        "\n",
        "                X_train_processed[col] = X_train_processed[col].fillna(mode_val)\n",
        "                X_val_processed[col] = X_val_processed[col].fillna(mode_val)\n",
        "\n",
        "        for col in self.numerical_cols:\n",
        "            if col in X_train_processed.columns:\n",
        "                if fit_preprocessors:\n",
        "                    median_val = X_train_processed[col].median()\n",
        "                    self.preprocessors[f'{col}_median'] = median_val\n",
        "                else:\n",
        "                    median_val = self.preprocessors[f'{col}_median']\n",
        "\n",
        "                X_train_processed[col] = X_train_processed[col].fillna(median_val)\n",
        "                X_val_processed[col] = X_val_processed[col].fillna(median_val)\n",
        "\n",
        "        # 2. ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\n",
        "        self.add_advanced_features(X_train_processed)\n",
        "        self.add_advanced_features(X_val_processed)\n",
        "\n",
        "        # 3. Target Encoding\n",
        "        remaining_categorical = [col for col in self.categorical_cols if col in X_train_processed.columns]\n",
        "\n",
        "        if remaining_categorical:\n",
        "            if fit_preprocessors:\n",
        "                self.preprocessors['target_encoder'] = TargetEncoder(smooth=1.0, random_state=42)\n",
        "                X_train_processed[remaining_categorical] = self.preprocessors['target_encoder'].fit_transform(\n",
        "                    X_train_processed[remaining_categorical], y_train\n",
        "                )\n",
        "            else:\n",
        "                X_train_processed[remaining_categorical] = self.preprocessors['target_encoder'].transform(\n",
        "                    X_train_processed[remaining_categorical]\n",
        "                )\n",
        "\n",
        "            X_val_processed[remaining_categorical] = self.preprocessors['target_encoder'].transform(\n",
        "                X_val_processed[remaining_categorical]\n",
        "            )\n",
        "\n",
        "        # 4. ìŠ¤ì¼€ì¼ë§\n",
        "        if fit_preprocessors:\n",
        "            self.preprocessors['scaler'] = RobustScaler()\n",
        "            X_train_scaled = pd.DataFrame(\n",
        "                self.preprocessors['scaler'].fit_transform(X_train_processed),\n",
        "                columns=X_train_processed.columns,\n",
        "                index=X_train_processed.index\n",
        "            )\n",
        "        else:\n",
        "            X_train_scaled = pd.DataFrame(\n",
        "                self.preprocessors['scaler'].transform(X_train_processed),\n",
        "                columns=X_train_processed.columns,\n",
        "                index=X_train_processed.index\n",
        "            )\n",
        "\n",
        "        X_val_scaled = pd.DataFrame(\n",
        "            self.preprocessors['scaler'].transform(X_val_processed),\n",
        "            columns=X_val_processed.columns,\n",
        "            index=X_val_processed.index\n",
        "        )\n",
        "\n",
        "        return X_train_scaled, X_val_scaled\n",
        "\n",
        "    def add_advanced_features(self, df):\n",
        "        \"\"\"ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\"\"\"\n",
        "        # 1. íš¨ìœ¨ì„± ì§€í‘œë“¤\n",
        "        if 'ì´ ìƒì„± ë°°ì•„ ìˆ˜' in df.columns and 'ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜' in df.columns:\n",
        "            df['ë°°ì•„_ìƒì„±_íš¨ìœ¨'] = df['ì´ ìƒì„± ë°°ì•„ ìˆ˜'] / (df['ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œìž ìˆ˜'] + 1)\n",
        "\n",
        "        if 'ì´ì‹ëœ ë°°ì•„ ìˆ˜' in df.columns and 'ì´ ìƒì„± ë°°ì•„ ìˆ˜' in df.columns:\n",
        "            df['ë°°ì•„_ì´ì‹_ë¹„ìœ¨'] = df['ì´ì‹ëœ ë°°ì•„ ìˆ˜'] / (df['ì´ ìƒì„± ë°°ì•„ ìˆ˜'] + 1)\n",
        "\n",
        "        if 'ì €ìž¥ëœ ë°°ì•„ ìˆ˜' in df.columns and 'ì´ ìƒì„± ë°°ì•„ ìˆ˜' in df.columns:\n",
        "            df['ë°°ì•„_ë³´ì¡´_ë¹„ìœ¨'] = df['ì €ìž¥ëœ ë°°ì•„ ìˆ˜'] / (df['ì´ ìƒì„± ë°°ì•„ ìˆ˜'] + 1)\n",
        "\n",
        "        # 2. ë¯¸ì„¸ì£¼ìž… ê´€ë ¨\n",
        "        if 'ë¯¸ì„¸ì£¼ìž…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜' in df.columns and 'ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜' in df.columns:\n",
        "            df['ë¯¸ì„¸ì£¼ìž…_ì„±ê³µë¥ '] = df['ë¯¸ì„¸ì£¼ìž…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜'] / (df['ë¯¸ì„¸ì£¼ìž…ëœ ë‚œìž ìˆ˜'] + 1)\n",
        "\n",
        "        # 3. ì¢…í•© ì ìˆ˜ë“¤\n",
        "        if 'ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´' in df.columns:\n",
        "            age_mapping = {\n",
        "                'ë§Œ18-34ì„¸': 1, 'ë§Œ35-37ì„¸': 2, 'ë§Œ38-39ì„¸': 3,\n",
        "                'ë§Œ40-42ì„¸': 4, 'ë§Œ43-44ì„¸': 5, 'ë§Œ45-50ì„¸': 6,\n",
        "                'ì•Œ ìˆ˜ ì—†ìŒ': 0\n",
        "            }\n",
        "            df['ë‚˜ì´_ê·¸ë£¹_ì ìˆ˜'] = df['ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´'].map(age_mapping).fillna(0)\n",
        "\n",
        "        # 4. ë¶ˆìž„ ì›ì¸ ì¢…í•©\n",
        "        infertility_cols = [col for col in df.columns if 'ë¶ˆìž„ ì›ì¸' in col and df[col].dtype in ['int64', 'float64']]\n",
        "        if infertility_cols:\n",
        "            df['ë¶ˆìž„_ì›ì¸_ì´ê°œìˆ˜'] = df[infertility_cols].sum(axis=1)\n",
        "\n",
        "        # 5. ë°°ì•„/ë‚œìž í’ˆì§ˆ ì§€í‘œ\n",
        "        if 'ì´ ìƒì„± ë°°ì•„ ìˆ˜' in df.columns and 'í˜¼í•©ëœ ë‚œìž ìˆ˜' in df.columns:\n",
        "            df['ìƒì‹ì„¸í¬_í™œìš©ë„'] = (df['ì´ ìƒì„± ë°°ì•„ ìˆ˜'] + df['í˜¼í•©ëœ ë‚œìž ìˆ˜']) / 2\n",
        "\n",
        "        # 6. ì¹˜ë£Œ ì´ë ¥ ì ìˆ˜\n",
        "        treatment_cols = [col for col in df.columns if 'íšŸìˆ˜' in col and df[col].dtype == 'object']\n",
        "        if treatment_cols:\n",
        "            treatment_score = 0\n",
        "            for col in treatment_cols:\n",
        "                # '0íšŒ', '1íšŒ' ë“±ì„ ìˆ«ìžë¡œ ë³€í™˜\n",
        "                numeric_vals = df[col].str.extract(r'(\\d+)').astype(float).fillna(0).iloc[:, 0]\n",
        "                treatment_score += numeric_vals\n",
        "            df['ì¹˜ë£Œ_ì´ë ¥_ì ìˆ˜'] = treatment_score\n",
        "\n",
        "# 3. ê³ ì„±ëŠ¥ ëª¨ë¸ë“¤ ì •ì˜\n",
        "models = {\n",
        "    'LightGBM_Tuned': lgb.LGBMClassifier(\n",
        "        random_state=42,\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.03,\n",
        "        num_leaves=63,\n",
        "        feature_fraction=0.8,\n",
        "        bagging_fraction=0.8,\n",
        "        min_child_samples=20,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.1,\n",
        "        verbose=-1\n",
        "    ),\n",
        "    'XGBoost_Tuned': xgb.XGBClassifier(\n",
        "        random_state=42,\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.03,\n",
        "        max_depth=8,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        min_child_weight=3,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.1,\n",
        "        eval_metric='logloss'\n",
        "    ),\n",
        "    'RandomForest_Tuned': RandomForestClassifier(\n",
        "        random_state=42,\n",
        "        n_estimators=500,\n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        max_features='sqrt',\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'ExtraTrees_Tuned': ExtraTreesClassifier(\n",
        "        random_state=42,\n",
        "        n_estimators=500,\n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        max_features='sqrt',\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'GradientBoosting_Tuned': GradientBoostingClassifier(\n",
        "        random_state=42,\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=8,\n",
        "        subsample=0.8,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=5\n",
        "    )\n",
        "}\n",
        "\n",
        "# 4. ê³ ê¸‰ êµì°¨ê²€ì¦ ì‹¤í–‰\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸš€ ê³ ì„±ëŠ¥ ëª¨ë¸ êµì°¨ê²€ì¦ ì‹œìž‘\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "cv_folds = 5\n",
        "skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "pipeline = AdvancedPreprocessingPipeline()\n",
        "\n",
        "model_scores = {}\n",
        "trained_models = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nðŸ”„ {name} í•™ìŠµ ì¤‘...\")\n",
        "\n",
        "    cv_scores = []\n",
        "    fold_models = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_clean, y_raw)):\n",
        "        # ë°ì´í„° ë¶„í• \n",
        "        X_train_fold = X_clean.iloc[train_idx]\n",
        "        X_val_fold = X_clean.iloc[val_idx]\n",
        "        y_train_fold = y_raw.iloc[train_idx]\n",
        "        y_val_fold = y_raw.iloc[val_idx]\n",
        "\n",
        "        # ì „ì²˜ë¦¬\n",
        "        X_train_processed, X_val_processed = pipeline.preprocess_data(\n",
        "            X_train_fold, X_val_fold, y_train_fold,\n",
        "            fit_preprocessors=(fold == 0)\n",
        "        )\n",
        "\n",
        "        # SMOTE ì ìš©\n",
        "        smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed, y_train_fold)\n",
        "\n",
        "        # ëª¨ë¸ í•™ìŠµ\n",
        "        model_copy = type(model)(**model.get_params())\n",
        "        model_copy.fit(X_train_balanced, y_train_balanced)\n",
        "        fold_models.append(model_copy)\n",
        "\n",
        "        # ì˜ˆì¸¡ ë° í‰ê°€\n",
        "        y_pred_proba = model_copy.predict_proba(X_val_processed)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_val_fold, y_pred_proba)\n",
        "\n",
        "        cv_scores.append(roc_auc)\n",
        "\n",
        "        if fold < 2:  # ì²˜ìŒ 2ê°œ foldë§Œ ì¶œë ¥ (ì†ë„ í–¥ìƒ)\n",
        "            print(f\"   Fold {fold+1}: {roc_auc:.4f}\")\n",
        "\n",
        "    avg_score = np.mean(cv_scores)\n",
        "    std_score = np.std(cv_scores)\n",
        "    model_scores[name] = {'mean': avg_score, 'std': std_score}\n",
        "    trained_models[name] = fold_models\n",
        "\n",
        "    print(f\"   âœ… í‰ê·  ROC-AUC: {avg_score:.4f} (Â±{std_score:.4f})\")\n",
        "\n",
        "# 5. ê²°ê³¼ ë¹„êµ\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ† ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "sorted_models = sorted(model_scores.items(), key=lambda x: x[1]['mean'], reverse=True)\n",
        "\n",
        "print(\"ðŸ“Š ROC-AUC ìˆœìœ„:\")\n",
        "for rank, (name, scores) in enumerate(sorted_models, 1):\n",
        "    print(f\"   {rank}. {name}: {scores['mean']:.4f} (Â±{scores['std']:.4f})\")\n",
        "\n",
        "# 6. ì•™ìƒë¸” êµ¬ì„± ë° ìµœì¢… ì˜ˆì¸¡\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ¤ ì•™ìƒë¸” ìµœì¢… ì˜ˆì¸¡\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ìƒìœ„ 3ê°œ ëª¨ë¸ë¡œ ì•™ìƒë¸”\n",
        "top_3_models = [name for name, _ in sorted_models[:3]]\n",
        "print(f\"ì•™ìƒë¸” êµ¬ì„±: {', '.join(top_3_models)}\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡\n",
        "print(\"ðŸ”„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\")\n",
        "\n",
        "final_predictions = []\n",
        "\n",
        "for fold in range(cv_folds):\n",
        "    # ê° foldì˜ ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
        "    pipeline_fold = AdvancedPreprocessingPipeline()\n",
        "\n",
        "    # ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° í•™ìŠµ\n",
        "    X_train_full_processed, _ = pipeline_fold.preprocess_data(\n",
        "        X_clean, X_clean.iloc[:100], y_raw, fit_preprocessors=True  # ë”ë¯¸ validation\n",
        "    )\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬\n",
        "    _, test_processed = pipeline_fold.preprocess_data(\n",
        "        X_clean, test_clean, y_raw, fit_preprocessors=False\n",
        "    )\n",
        "\n",
        "    # ê° ëª¨ë¸ì˜ foldë³„ ì˜ˆì¸¡\n",
        "    fold_predictions = []\n",
        "    for model_name in top_3_models:\n",
        "        model = trained_models[model_name][fold]\n",
        "        pred_proba = model.predict_proba(test_processed)[:, 1]\n",
        "        fold_predictions.append(pred_proba)\n",
        "\n",
        "    # ê°€ì¤‘ í‰ê·  (ì„±ëŠ¥ ê¸°ë°˜)\n",
        "    weights = [model_scores[name]['mean'] for name in top_3_models]\n",
        "    weights = np.array(weights) / sum(weights)\n",
        "\n",
        "    ensemble_pred = np.average(fold_predictions, axis=0, weights=weights)\n",
        "    final_predictions.append(ensemble_pred)\n",
        "\n",
        "# foldë³„ ì˜ˆì¸¡ì˜ í‰ê· \n",
        "final_ensemble_pred = np.mean(final_predictions, axis=0)\n",
        "\n",
        "# 7. ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ“„ ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "test_ids = pd.read_csv('./test.csv')['ID']\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_ids,\n",
        "    'probability': final_ensemble_pred\n",
        "})\n",
        "\n",
        "submission_filename = 'final_high_performance_submission.csv'\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"âœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: {submission_filename}\")\n",
        "print(f\"   ìƒ˜í”Œ ìˆ˜: {len(submission):,}ê°œ\")\n",
        "print(f\"   ì˜ˆì¸¡ í™•ë¥  ë²”ìœ„: {final_ensemble_pred.min():.4f} ~ {final_ensemble_pred.max():.4f}\")\n",
        "print(f\"   í‰ê·  ì˜ˆì¸¡ í™•ë¥ : {final_ensemble_pred.mean():.4f}\")\n",
        "\n",
        "# 8. ìµœì¢… ì„±ê³¼ ìš”ì•½\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸŽ¯ ìµœì¢… ì„±ê³¼ ìš”ì•½\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "best_score = sorted_models[0][1]['mean']\n",
        "baseline_estimated = 0.55  # ë² ì´ìŠ¤ë¼ì¸ ExtraTreesClassifier ì¶”ì •ì¹˜\n",
        "\n",
        "improvement = ((best_score - baseline_estimated) / baseline_estimated) * 100\n",
        "\n",
        "print(f\"ðŸš€ ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„ ì‚¬í•­:\")\n",
        "print(f\"   ðŸ“ˆ ë² ì´ìŠ¤ë¼ì¸ ì¶”ì • ì„±ëŠ¥: {baseline_estimated:.3f}\")\n",
        "print(f\"   ðŸ† ìµœì¢… ëª¨ë¸ ì„±ëŠ¥: {best_score:.4f}\")\n",
        "print(f\"   ðŸ“Š ì„±ëŠ¥ í–¥ìƒ: +{improvement:.1f}%\")\n",
        "print(f\"   ðŸ¤– ìµœì¢… ë°©ë²•: ìƒìœ„ 3ê°œ ëª¨ë¸ ê°€ì¤‘ ì•™ìƒë¸”\")\n",
        "\n",
        "print(f\"\\nðŸ”¬ ê¸°ìˆ ì  ê°œì„ ì‚¬í•­:\")\n",
        "print(f\"   âœ… ë°ì´í„° ë¦¬í‚¤ì§€ ì™„ì „ ë°©ì§€\")\n",
        "print(f\"   âœ… ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (7ê°œ ë„ë©”ì¸ íŠ¹ì„±)\")\n",
        "print(f\"   âœ… Target Encoding + RobustScaler\")\n",
        "print(f\"   âœ… SMOTE ë¶ˆê· í˜• í•´ê²°\")\n",
        "print(f\"   âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\")\n",
        "print(f\"   âœ… 5-Fold êµì°¨ê²€ì¦\")\n",
        "print(f\"   âœ… ê°€ì¤‘ ì•™ìƒë¸”\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸŽ‰ ìµœì¢… ê³ ì„±ëŠ¥ ëª¨ë¸ ì™„ì„±!\")\n",
        "print(f\"ðŸ“ ì œì¶œ íŒŒì¼: {submission_filename}\")\n",
        "print(\"ðŸ† ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ëŒ€í­ ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„±!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJnbuxmVGFQV",
        "outputId": "f4ed65cc-9bcb-4e85-e427-2393bd7390de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ðŸ† ìµœì¢… ê³ ì„±ëŠ¥ ëª¨ë¸ - ì™„ì „ì²´ ì‹œìž‘\n",
            "======================================================================\n",
            "ðŸ“‚ ë°ì´í„° ì¤€ë¹„...\n",
            "âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: (256351, 61)\n",
            "\n",
            "==================================================\n",
            "ðŸš€ ê³ ì„±ëŠ¥ ëª¨ë¸ êµì°¨ê²€ì¦ ì‹œìž‘\n",
            "==================================================\n",
            "\n",
            "ðŸ”„ LightGBM_Tuned í•™ìŠµ ì¤‘...\n",
            "   Fold 1: 0.7001\n",
            "   Fold 2: 0.7399\n",
            "   âœ… í‰ê·  ROC-AUC: 0.7297 (Â±0.0149)\n",
            "\n",
            "ðŸ”„ XGBoost_Tuned í•™ìŠµ ì¤‘...\n",
            "   Fold 1: 0.6949\n",
            "   Fold 2: 0.7387\n",
            "   âœ… í‰ê·  ROC-AUC: 0.7277 (Â±0.0165)\n",
            "\n",
            "ðŸ”„ RandomForest_Tuned í•™ìŠµ ì¤‘...\n",
            "   Fold 1: 0.7284\n",
            "   Fold 2: 0.7349\n",
            "   âœ… í‰ê·  ROC-AUC: 0.7316 (Â±0.0021)\n",
            "\n",
            "ðŸ”„ ExtraTrees_Tuned í•™ìŠµ ì¤‘...\n",
            "   Fold 1: 0.7280\n",
            "   Fold 2: 0.7345\n",
            "   âœ… í‰ê·  ROC-AUC: 0.7309 (Â±0.0022)\n",
            "\n",
            "ðŸ”„ GradientBoosting_Tuned í•™ìŠµ ì¤‘...\n"
          ]
        }
      ]
    }
  ]
}